{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "regulated-offense",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility. Expected 80 from C header, got 96 from PyObject\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216 from C header, got 232 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import integrate, interpolate, stats\n",
    "from scipy.interpolate import InterpolatedUnivariateSpline\n",
    "from scipy.integrate import quad\n",
    "\n",
    "import healpy as hp\n",
    "from astropy.io import fits\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy import units as u\n",
    "from astropy.cosmology import Planck18 as cosmo\n",
    "from astropy.table import Table\n",
    "\n",
    "from nbodykit.lab import *\n",
    "from nbodykit import setup_logging, style\n",
    "\n",
    "from pypower import CatalogMesh, MeshFFTPower, CatalogFFTPower, PowerSpectrumStatistics, utils, PowerSpectrumWedges\n",
    "import sympy as sp\n",
    "from sympy import Symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4111d4e-605e-425d-8236-b8fc5ae7dedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,7)\n",
    "plt.rcParams[\"font.size\"] = 20\n",
    "plt.rcParams[\"font.family\"]='serif'\n",
    "plt.rcParams['text.usetex']=True\n",
    "plt.rcParams['axes.linewidth'] = 1.5\n",
    "plt.rcParams['figure.dpi'] = 600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f684566-16a8-475c-bae8-01dacaa28be8",
   "metadata": {},
   "source": [
    "# Functions used for modeling \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-burden",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skymap(cat, nside=8):\n",
    "    \"\"\"\n",
    "    Generates a Healpix map with redshift as values for each pixel.\n",
    "    This is useful for visualizing the redshift distribution of sources across the sky.\n",
    "    \n",
    "    Parameters:\n",
    "        cat: catalog of Right Ascension and Declination in degrees.\n",
    "        nside: resolution.\n",
    "        \n",
    "    Returns:\n",
    "        m (np.ndarray): 1D array with redshift values for each HEALPix pixel.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Convert RA/Dec to galactic coordinates for uniform sky mapping\n",
    "    c = SkyCoord(ra=np.asarray(cat.ra) * u.degree, dec=np.asarray(cat.dec) * u.degree, frame='icrs', unit='deg')\n",
    "    l, b = c.galactic.l.value, c.galactic.b.value\n",
    "    \n",
    "    # HEALPix expects theta/phi in radians\n",
    "    theta = np.radians(90. - b)\n",
    "    phi = np.radians(l)\n",
    "    \n",
    "    # Assign each object to a HEALPix pixel\n",
    "    pixel_indices = hp.ang2pix(nside, theta, phi)\n",
    "    \n",
    "    # Initialize map: each pixel will hold the redshift of the object(s) in that pixel\n",
    "    m = np.zeros(hp.nside2npix(nside))\n",
    "    \n",
    "    m[pixel_indices] = cat.z  # This will overwrite if multiple objects fall in the same pixel\n",
    "    \n",
    "    return m\n",
    "\n",
    "def countmap(Ra, Dec, nside):\n",
    "    \"\"\"\n",
    "    Create a HEALPix map counting the number of objects per pixel from RA/Dec coordinates.\n",
    "\n",
    "    Parameters:\n",
    "        Ra (array-like): Right Ascension values in degrees.\n",
    "        Dec (array-like): Declination values in degrees.\n",
    "        nside (int): HEALPix NSIDE parameter (resolution).\n",
    "\n",
    "    Returns:\n",
    "        hpx_map (np.ndarray): 1D array with counts per HEALPix pixel.\n",
    "    \"\"\"\n",
    "    # Convert RA/Dec to galactic coordinates\n",
    "    c = SkyCoord(ra=np.asarray(Ra) * u.degree, dec=np.asarray(Dec) * u.degree, frame='icrs', unit='deg')\n",
    "    l, b = c.galactic.l.value, c.galactic.b.value\n",
    "\n",
    "    # Convert to HEALPix theta/phi\n",
    "    theta = np.radians(90. - b)\n",
    "    phi = np.radians(l)\n",
    "\n",
    "    # Number of pixels in the map\n",
    "    npix = hp.nside2npix(nside)\n",
    "\n",
    "    # Get HEALPix pixel indices for each object\n",
    "    indices = hp.ang2pix(nside, theta, phi)\n",
    "    idx, counts = np.unique(indices, return_counts=True)\n",
    "\n",
    "    # Create map and fill with counts\n",
    "    hpx_map = np.zeros(npix)\n",
    "    hpx_map[idx] = counts\n",
    "    return hpx_map\n",
    "\n",
    "def indices(Ra, Dec, nside=256):\n",
    "    \"\"\"\n",
    "    Get HEALPix pixel indices for given RA/Dec coordinates.\n",
    "\n",
    "    Parameters:\n",
    "        Ra (array-like): Right Ascension values in degrees.\n",
    "        Dec (array-like): Declination values in degrees.\n",
    "        nside (int): HEALPix NSIDE parameter (default 256).\n",
    "\n",
    "    Returns:\n",
    "        indices (np.ndarray): Array of HEALPix pixel indices.\n",
    "    \"\"\"\n",
    "    # Convert RA/Dec to galactic coordinates\n",
    "    c = SkyCoord(ra=np.asarray(Ra) * u.degree, dec=np.asarray(Dec) * u.degree, frame='icrs', unit='deg')\n",
    "    l, b = c.galactic.l.value, c.galactic.b.value\n",
    "\n",
    "    # Convert to HEALPix theta/phi\n",
    "    theta = np.radians(90. - b)\n",
    "    phi = np.radians(l)\n",
    "\n",
    "    # Get HEALPix pixel indices\n",
    "    indices = hp.ang2pix(nside, theta, phi)\n",
    "    return indices\n",
    "\n",
    "def weighted_countmap(Ra, Dec, weights, nside=256):\n",
    "    \"\"\"\n",
    "    Create a HEALPix map summing weights per pixel from RA/Dec coordinates.\n",
    "\n",
    "    Parameters:\n",
    "        Ra (array-like): Right Ascension values in degrees.\n",
    "        Dec (array-like): Declination values in degrees.\n",
    "        weights (array-like): Weights for each object.\n",
    "        nside (int): HEALPix NSIDE parameter (default 256).\n",
    "\n",
    "    Returns:\n",
    "        hpx_map (np.ndarray): 1D array with weighted sum per HEALPix pixel.\n",
    "    \"\"\"\n",
    "    # Convert RA/Dec to galactic coordinates\n",
    "    c = SkyCoord(ra=np.asarray(Ra) * u.degree, dec=np.asarray(Dec) * u.degree, frame='icrs', unit='deg')\n",
    "    l, b = c.galactic.l.value, c.galactic.b.value\n",
    "\n",
    "    # Convert to HEALPix theta/phi\n",
    "    theta = np.radians(90. - b)\n",
    "    phi = np.radians(l)\n",
    "\n",
    "    # Number of pixels in the map\n",
    "    npix = hp.nside2npix(nside)\n",
    "\n",
    "    # Get HEALPix pixel indices for each object\n",
    "    indices = hp.ang2pix(nside, theta, phi)\n",
    "\n",
    "    # Initialize the HEALPix map\n",
    "    hpx_map = np.zeros(npix)\n",
    "\n",
    "    # Sum weights for each pixel\n",
    "    for pix, w in zip(indices, weights):\n",
    "        hpx_map[pix] += w\n",
    "\n",
    "    return hpx_map\n",
    "\n",
    "\n",
    "def f_gotmap(Ra,Dec,weights,nside=256):\n",
    "    \"\"\"\n",
    "    Calculate the average weight map (f_gotmap) from RA/Dec coordinates.\n",
    "\n",
    "    Parameters:\n",
    "        Ra (array-like): Right Ascension values in degrees.\n",
    "        Dec (array-like): Declination values in degrees.\n",
    "        weights (array-like): Weights for each object.\n",
    "        nside (int): HEALPix NSIDE parameter (default 256).\n",
    "\n",
    "    Returns:\n",
    "        ave (np.ndarray): 1D array with average weights per HEALPix pixel.\n",
    "    \"\"\"\n",
    "    # Calculate the average weight map\n",
    "    ave = weighted_countmap(Ra,Dec,weights, nside =256) / countmap(Ra,Dec, nside =256)\n",
    "    return ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "def densitymap(cat, npix_nz, nside=8):\n",
    "    \"\"\"\n",
    "    Create a HEALPix density map from a catalog of objects.\n",
    "\n",
    "    Parameters:\n",
    "        cat (pandas.DataFrame): Catalog with 'ra' and 'dec' columns (in degrees).\n",
    "        npix_nz (int): Number of non-zero pixels (used for normalization).\n",
    "        nside (int): HEALPix NSIDE parameter (default 8).\n",
    "\n",
    "    Returns:\n",
    "        hpx_map (np.ndarray): 1D array with density contrast per HEALPix pixel.\n",
    "    \"\"\"\n",
    "    # Convert RA/Dec to galactic coordinates\n",
    "    c = SkyCoord(ra=np.asarray(cat.ra) * u.degree, dec=np.asarray(cat.dec) * u.degree, frame='icrs', unit='deg')\n",
    "    l, b = c.galactic.l.value, c.galactic.b.value\n",
    "    theta = np.radians(90. - b)\n",
    "    phi = np.radians(l)\n",
    "\n",
    "    # Number of pixels in the map\n",
    "    npix = hp.nside2npix(nside)\n",
    "\n",
    "    # Get HEALPix pixel indices for each object\n",
    "    pixel_indices = hp.ang2pix(nside, theta, phi)\n",
    "    N_z = len(cat)\n",
    "    N_bar = N_z / npix_nz  # Mean number per pixel\n",
    "    print(N_z, npix_nz, N_bar)\n",
    "\n",
    "    indices = hp.ang2pix(nside, theta, phi)\n",
    "    idx, counts = np.unique(indices, return_counts=True)\n",
    "\n",
    "    # Initialize the map with -1 for empty regions\n",
    "    hpx_map = -np.ones(npix)\n",
    "    # Fill with density contrast (counts/N_bar - 1)\n",
    "    hpx_map[idx] = counts / N_bar - 1\n",
    "    return hpx_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f03f291-79d7-47c2-9c33-977b4f587407",
   "metadata": {},
   "outputs": [],
   "source": [
    "H_0 = 70  # Hubble constant in km/s/Mpc\n",
    "H_0_inv = 13.98e9  # Inverse Hubble constant in years\n",
    "# If we ever want to plot P_t_d, make sure to change this appropriately to the cosmology\n",
    "\n",
    "def normalize_pdf(pdf, x_arr):\n",
    "    \"\"\"\n",
    "    Normalize a probability density function (PDF) over a given array of x values.\n",
    "\n",
    "    Parameters:\n",
    "        pdf (array-like): Probability density values.\n",
    "        x_arr (array-like): Corresponding x values.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Normalized PDF.\n",
    "    \"\"\"\n",
    "    normalization = np.trapz(pdf, x_arr)  # Integrate PDF over x_arr\n",
    "    return pdf / normalization\n",
    "\n",
    "def rejection_sampling(N, p, x_low, x_high, y_max, p_args=[]):\n",
    "    \"\"\"\n",
    "    Perform rejection sampling to draw samples from a probability distribution.\n",
    "\n",
    "    Parameters:\n",
    "        N (int): Number of samples to generate.\n",
    "        p (callable): Probability distribution function to sample from.\n",
    "        x_low (float): Lower bound of x range.\n",
    "        x_high (float): Upper bound of x range.\n",
    "        y_max (float): Maximum value of the PDF in the range (can be an overestimate).\n",
    "        p_args (list): Additional arguments for the PDF function.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of sampled points.\n",
    "    \"\"\"\n",
    "    sampled_points = []\n",
    "    for i in range(N):\n",
    "        accepted = False\n",
    "        while not accepted:\n",
    "            x = np.random.uniform(x_low, x_high)\n",
    "            y = np.random.uniform(0, y_max)\n",
    "            p_x = p(x, *p_args)\n",
    "            if y <= p_x:\n",
    "                accepted = True\n",
    "                sampled_points.append(x)\n",
    "    return np.array(sampled_points)\n",
    "\n",
    "def rejection_sampling_numeric(N, m_array, p_m_array):\n",
    "    \"\"\"\n",
    "    Perform rejection sampling using a numeric PDF array instead of a function.\n",
    "\n",
    "    Parameters:\n",
    "        N (int): Number of samples to generate.\n",
    "        m_array (array-like): Sorted array of x values.\n",
    "        p_m_array (array-like): PDF values corresponding to m_array.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of sampled points.\n",
    "    \"\"\"\n",
    "    y_max = max(p_m_array)\n",
    "    x_low, x_high = min(m_array), max(m_array)\n",
    "    sampled_points = []\n",
    "    for i in range(N):\n",
    "        accepted = False\n",
    "        while not accepted:\n",
    "            x = np.random.uniform(x_low, x_high)\n",
    "            y = np.random.uniform(0, y_max)\n",
    "            # Find PDF value for x using searchsorted\n",
    "            p_x = p_m_array[np.searchsorted(m_array, x, side=\"left\")]\n",
    "            if y <= p_x:\n",
    "                accepted = True\n",
    "                sampled_points.append(x)\n",
    "    return np.array(sampled_points)\n",
    "\n",
    "def P_t_d(t_d, kappa=1, t_min=100e6, norm=True):\n",
    "    \"\"\"\n",
    "    Delay time distribution: P(t_d) âˆ t_d^(-kappa) for t_d >= t_min, 0 otherwise.\n",
    "\n",
    "    Parameters:\n",
    "        t_d (float or array): Delay time(s) in years.\n",
    "        kappa (float): Power-law index.\n",
    "        t_min (float): Minimum delay time (years).\n",
    "        norm (bool): If True, normalize the distribution.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Probability density values.\n",
    "    \"\"\"\n",
    "    f_x = 0\n",
    "    if norm:\n",
    "        # Normalized power-law with cutoff at t_min\n",
    "        f_x = np.piecewise(t_d, [t_d < t_min, t_d >= t_min], [0, lambda t_d: 1/(np.log(H_0_inv/t_min)) * t_d**(-kappa)])\n",
    "    else:\n",
    "        f_x = np.piecewise(t_d, [t_d < t_min, t_d >= t_min], [0, lambda t_d: t_d**(-kappa)])\n",
    "    return f_x\n",
    "\n",
    "def SFR(z):\n",
    "    \"\"\"\n",
    "    Madau-Dickinson star formation rate (SFR) as a function of redshift z.\n",
    "\n",
    "    Parameters:\n",
    "        z (float or array): Redshift(s).\n",
    "\n",
    "    Returns:\n",
    "        float or np.ndarray: SFR in units of M_sun yr^-1 Mpc^-3.\n",
    "    \"\"\"\n",
    "    return 0.015 * (1+z)**(2.7) / (1 + ((1+z)/2.9)**(5.6)) * 1e9\n",
    "\n",
    "def R_integrand(z, t_merg, t_min=100e6, kappa=1):\n",
    "    \"\"\"\n",
    "    Integrand for merger rate calculation as a function of redshift.\n",
    "\n",
    "    Parameters:\n",
    "        z (float): Redshift.\n",
    "        t_merg (float): Merger time in years.\n",
    "        t_min (float): Minimum delay time (years).\n",
    "        kappa (float): Power-law index for delay time distribution.\n",
    "\n",
    "    Returns:\n",
    "        float: Value of the integrand.\n",
    "    \"\"\"\n",
    "    t_d = cosmo.lookback_time(z).to_value('year') - t_merg\n",
    "    return P_t_d(t_d, kappa=kappa, t_min=t_min) * cosmo.lookback_time_integrand(z) * SFR(z)\n",
    "\n",
    "# def merger_rate(z, t_min=500e6, kappa=1):\n",
    "#     return integrate.quad(R_integrand, z, 1e4, args=( cosmo.lookback_time(z).to_value('year'), t_min, kappa ), limit=300)[0]\n",
    "\n",
    "# def merger_rate(z_arr, t_min=500e6, kappa=1):\n",
    "#     '''\n",
    "#     Computes merger rate ... Can optimize this code\n",
    "#     '''\n",
    "#     if isinstance(z_arr, np.ndarray):\n",
    "#         R_merger = []\n",
    "#         for z in z_arr:\n",
    "#             R_merger.append((integrate.quad(R_integrand, z, 1e4, args=( cosmo.lookback_time(z).to_value('year'), 500e6,), limit=300))[0])\n",
    "#         R_merger = np.array(R_merger)\n",
    "#         return R_merger\n",
    "#     return integrate.quad(R_integrand, z_arr, 1e4, args=( cosmo.lookback_time(z_arr).to_value('year'), t_min, kappa ), limit=300)[0]\n",
    "\n",
    "\n",
    "def merger_rate(z, t_min=500e6, kappa=1):\n",
    "    \"\"\"\n",
    "    Compute the merger rate at a given redshift by integrating the delay time distribution.\n",
    "\n",
    "    Parameters:\n",
    "        z (float): Redshift.\n",
    "        t_min (float): Minimum delay time (years).\n",
    "        kappa (float): Power-law index.\n",
    "\n",
    "    Returns:\n",
    "        float: Merger rate.\n",
    "    \"\"\"\n",
    "    return integrate.quad(R_integrand, z, np.inf, args=(cosmo.lookback_time(z).to_value('year'), t_min, kappa), limit=300)[0]\n",
    "\n",
    "def N_GW_integrand(z, interp_rate, obs_time, t_min=500e6, kappa=1):\n",
    "    \"\"\"\n",
    "    Integrand for the expected number of gravitational wave events as a function of redshift.\n",
    "\n",
    "    Parameters:\n",
    "        z (float): Redshift.\n",
    "        interp_rate (callable): Interpolated merger rate function.\n",
    "        obs_time (float): Observation time (years).\n",
    "        t_min (float): Minimum delay time (years).\n",
    "        kappa (float): Power-law index.\n",
    "\n",
    "    Returns:\n",
    "        float: Value of the integrand.\n",
    "    \"\"\"\n",
    "    dVdz = 4 * np.pi * cosmo.differential_comoving_volume(z).to_value('Gpc3 / sr')\n",
    "    return dVdz * interp_rate(z) / (1 + z) * obs_time\n",
    "\n",
    "def N_GW_integrand_test(z, interp_rate, obs_time, t_min=500e6, kappa=1):\n",
    "    \"\"\"\n",
    "    Alternative integrand for the expected number of GW events, using a different rate normalization.\n",
    "\n",
    "    Parameters:\n",
    "        z (float): Redshift.\n",
    "        interp_rate (callable): Interpolated merger rate function.\n",
    "        obs_time (float): Observation time (years).\n",
    "        t_min (float): Minimum delay time (years).\n",
    "        kappa (float): Power-law index.\n",
    "\n",
    "    Returns:\n",
    "        float: Value of the integrand.\n",
    "    \"\"\"\n",
    "    dVdz = 4 * np.pi * cosmo.differential_comoving_volume(z).to_value('Gpc3 / sr')\n",
    "    rate = interp_rate(z, t_min=t_min, kappa=1)\n",
    "    return dVdz * rate * 23.9 / interp_rate(0, t_min=t_min, kappa=1) / (1 + z) * obs_time\n",
    "\n",
    "def N_GW_bin(z_min, z_max, obs_time, t_min=500e6, kappa=1, grid_points=1999):\n",
    "    \"\"\"\n",
    "    Compute the expected number of GW events in a redshift bin using an interpolated merger rate.\n",
    "\n",
    "    Parameters:\n",
    "        z_min (float): Minimum redshift.\n",
    "        z_max (float): Maximum redshift.\n",
    "        obs_time (float): Observation time (years).\n",
    "        t_min (float): Minimum delay time (years).\n",
    "        kappa (float): Power-law index.\n",
    "        grid_points (int): Number of grid points for interpolation.\n",
    "\n",
    "    Returns:\n",
    "        float: Expected number of GW events in the bin.\n",
    "    \"\"\"\n",
    "    z_arr = np.logspace(-10, np.log10(10), grid_points)\n",
    "    z_arr = np.insert(z_arr, 0, 0)\n",
    "    merger_rate_vec = np.vectorize(merger_rate)\n",
    "    rate = merger_rate_vec(z_arr, t_min=t_min, kappa=kappa)\n",
    "    norm_rate = rate * 23.9 / rate[0]\n",
    "    norm_rate_fn = interpolate.interp1d(z_arr, norm_rate)\n",
    "    return integrate.quad(N_GW_integrand, z_min, z_max, args=(norm_rate_fn, obs_time, t_min, kappa), limit=300)[0]\n",
    "\n",
    "def N_GW_bin_no_interp(z_min, z_max, obs_time, t_min=500e6, kappa=1, grid_points=1999):\n",
    "    \"\"\"\n",
    "    Compute the expected number of GW events in a redshift bin without interpolation.\n",
    "\n",
    "    Parameters:\n",
    "        z_min (float): Minimum redshift.\n",
    "        z_max (float): Maximum redshift.\n",
    "        obs_time (float): Observation time (years).\n",
    "        t_min (float): Minimum delay time (years).\n",
    "        kappa (float): Power-law index.\n",
    "        grid_points (int): Number of grid points for interpolation.\n",
    "\n",
    "    Returns:\n",
    "        float: Expected number of GW events in the bin.\n",
    "    \"\"\"\n",
    "    merger_rate_vec = np.vectorize(merger_rate)\n",
    "    return integrate.quad(N_GW_integrand_test, z_min, z_max, args=(merger_rate_vec, obs_time, t_min, kappa), limit=300)[0]\n",
    "\n",
    "\n",
    "def metallicity(C1, C2):\n",
    "    \"\"\"\n",
    "    Compute the average metallicity from two metallicity indicators.\n",
    "\n",
    "    Parameters:\n",
    "        C1 (array-like or float): First metallicity indicator.\n",
    "        C2 (array-like or float): Second metallicity indicator.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray or float: The average metallicity value(s).\n",
    "    \"\"\"\n",
    "    # Take the mean of the two metallicity indicators\n",
    "    return np.array((C1 + C2) / 2)\n",
    "\n",
    "\n",
    "def P_g_full(g, power_p, power_n, Break, low_g=None, high_g=None):\n",
    "    \"\"\"\n",
    "    Compute a broken power-law probability distribution for a galaxy property (e.g., mass, SFR, metallicity).\n",
    "\n",
    "    The distribution follows a power law with exponent 'power_p' below the break value, and 'power_n' above it.\n",
    "    Optionally, lower and upper cut-offs can be applied.\n",
    "\n",
    "    Parameters:\n",
    "        g (array-like): Property values (e.g., mass, SFR, metallicity).\n",
    "        power_p (float): Power-law exponent below the break.\n",
    "        power_n (float): Power-law exponent above the break.\n",
    "        Break (float): Break value separating the two regimes.\n",
    "        low_g (float, optional): Lower cut-off for g.\n",
    "        high_g (float, optional): Upper cut-off for g.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Normalized probability distribution for g.\n",
    "    \"\"\"\n",
    "    g = np.array(g)\n",
    "    prob = np.zeros(len(g))\n",
    "    for i in range(len(g)):\n",
    "        # Below the break, use power_p exponent\n",
    "        if g[i] < Break:\n",
    "            prob[i] = g[i] ** power_p\n",
    "        # Above the break, use power_n exponent and normalize at the break\n",
    "        if g[i] >= Break:\n",
    "            prob[i] = g[i] ** power_n / Break ** (power_n - power_p)\n",
    "    # Apply lower and upper cut-offs if specified\n",
    "    if low_g:\n",
    "        prob = prob * np.array([0 if g < low_g else 1 for g in g])\n",
    "    if high_g:\n",
    "        prob = prob * np.array([0 if g > high_g else 1 for g in g])\n",
    "    # Normalize the probability distribution\n",
    "    return prob / np.sum(prob)\n",
    "\n",
    "\n",
    "def P_g_unconditional(\n",
    "    M, sfr, metal=None,\n",
    "    m_power_p=None, m_power_n=None, m_break=None, min_m=None, max_m=None,\n",
    "    SFR_power_p=None, SFR_power_n=None, SFR_break=None, min_SFR=None, max_SFR=None,\n",
    "    metal_power_p=None, metal_power_n=None, metal_break=None, min_metal=None, max_metal=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the unconditional joint probability distribution for galaxy properties (mass, SFR, metallicity).\n",
    "\n",
    "    The distribution is the product of individual (possibly broken power-law) distributions for each property.\n",
    "    If metallicity is not provided, only mass and SFR are used.\n",
    "\n",
    "    Parameters:\n",
    "        M (array-like): Stellar mass.\n",
    "        sfr (array-like): Star formation rate.\n",
    "        metal (array-like, optional): Metallicity.\n",
    "        m_power_p, m_power_n, m_break, min_m, max_m: Parameters for mass distribution.\n",
    "        SFR_power_p, SFR_power_n, SFR_break, min_SFR, max_SFR: Parameters for SFR distribution.\n",
    "        metal_power_p, metal_power_n, metal_break, min_metal, max_metal: Parameters for metallicity distribution.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Normalized joint probability distribution.\n",
    "    \"\"\"\n",
    "    if metal is None:\n",
    "        # Product of mass and SFR distributions\n",
    "        p = P_g_full(M, m_power_p, m_power_n, m_break, min_m, max_m) * \\\n",
    "            P_g_full(sfr, SFR_power_p, SFR_power_n, SFR_break, min_SFR, max_SFR)\n",
    "    else:\n",
    "        # Product of mass, SFR, and metallicity distributions\n",
    "        p = P_g_full(M, m_power_p, m_power_n, m_break, min_m, max_m) * \\\n",
    "            P_g_full(sfr, SFR_power_p, SFR_power_n, SFR_break, min_SFR, max_SFR) * \\\n",
    "            P_g_full(metal, metal_power_p, metal_power_n, metal_break, min_metal, max_metal)\n",
    "    # Normalize the joint probability\n",
    "    p = p / np.sum(p)\n",
    "    return p\n",
    "\n",
    "\n",
    "def P_MZR(M, delta_l, delta_h, M_c, low_M=None, high_M=None):\n",
    "    \"\"\"\n",
    "    Compute the probability distribution for the Mass-Metallicity Relation (MZR).\n",
    "\n",
    "    This is modeled as a broken power-law in stellar mass, with a break at M_c.\n",
    "\n",
    "    Parameters:\n",
    "        M (array-like): Stellar mass.\n",
    "        delta_l (float): Power-law slope below the break (low mass end).\n",
    "        delta_h (float): Power-law slope above the break (high mass end).\n",
    "        M_c (float): Break mass.\n",
    "        low_M (float, optional): Lower cut-off for mass.\n",
    "        high_M (float, optional): Upper cut-off for mass.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Probability distribution for MZR.\n",
    "    \"\"\"\n",
    "    # Use P_g_full with appropriate exponents for the broken power-law\n",
    "    return P_g_full(M, 1/delta_l, -1/delta_h, M_c, low_M, high_M)\n",
    "\n",
    "\n",
    "def P_FMR(M, sfr, delta_h, M_c, low_M, high_M, epsilon_h, sfr_c, low_sfr, high_sfr):\n",
    "    \"\"\"\n",
    "    Compute the probability distribution for the Fundamental Metallicity Relation (FMR).\n",
    "\n",
    "    The FMR is modeled as the product of two broken power-law distributions:\n",
    "    one in mass (M) and one in SFR, each with their own break and slope.\n",
    "\n",
    "    Parameters:\n",
    "        M (array-like): Stellar mass.\n",
    "        sfr (array-like): Star formation rate.\n",
    "        delta_h (float): Slope above the break for mass.\n",
    "        M_c (float): Break mass.\n",
    "        low_M, high_M (float): Mass cut-offs.\n",
    "        epsilon_h (float): Slope above the break for SFR.\n",
    "        sfr_c (float): Break SFR.\n",
    "        low_sfr, high_sfr (float): SFR cut-offs.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Probability distribution for FMR.\n",
    "    \"\"\"\n",
    "    # Product of mass and SFR broken power-law distributions\n",
    "    # Note: normalization is handled later in the workflow\n",
    "    return P_g_full(M, 0, -1/delta_h, M_c, low_M, high_M) * \\\n",
    "           P_g_full(sfr, 0, -1/epsilon_h, sfr_c, low_sfr, high_sfr)\n",
    "\n",
    "\n",
    "def P_MZSFR(\n",
    "    M, sfr, Z,\n",
    "    delta_l, delta_h, M_c, low_M, high_M,\n",
    "    epsilon_l, epsilon_h, sfr_c, low_sfr, high_sfr,\n",
    "    zeta_l, zeta_h, Z_c, low_Z, high_Z\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the joint probability distribution for the Mass-Z-SFR relation (MZSFR).\n",
    "\n",
    "    This is modeled as the product of three broken power-law distributions:\n",
    "    one each for mass (M), SFR, and metallicity (Z), with their own breaks and slopes.\n",
    "\n",
    "    Parameters:\n",
    "        M (array-like): Stellar mass.\n",
    "        sfr (array-like): Star formation rate.\n",
    "        Z (array-like): Metallicity.\n",
    "        delta_l, delta_h (float): Slopes for mass below/above break.\n",
    "        M_c (float): Break mass.\n",
    "        low_M, high_M (float): Mass cut-offs.\n",
    "        epsilon_l, epsilon_h (float): Slopes for SFR below/above break.\n",
    "        sfr_c (float): Break SFR.\n",
    "        low_sfr, high_sfr (float): SFR cut-offs.\n",
    "        zeta_l, zeta_h (float): Slopes for Z below/above break.\n",
    "        Z_c (float): Break metallicity.\n",
    "        low_Z, high_Z (float): Metallicity cut-offs.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Probability distribution for MZSFR.\n",
    "    \"\"\"\n",
    "    # Product of mass, SFR, and metallicity broken power-law distributions\n",
    "    # Note: normalization is handled later in the workflow\n",
    "    return P_g_full(M, 1/delta_l, -1/delta_h, M_c, low_M, high_M) * \\\n",
    "           P_g_full(sfr, 1/epsilon_l, -1/epsilon_h, sfr_c, low_sfr, high_sfr) * \\\n",
    "           P_g_full(Z, 1/zeta_l, -1/zeta_h, Z_c, low_Z, high_Z)\n",
    "\n",
    "def g_selection_full(df,N,p,p_args,SFR_Z_type):\n",
    "    \"\"\"\n",
    "    Select a subsample of galaxies based on a probability distribution function (PDF) and given parameters.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): Input data containing galaxy properties.\n",
    "        N (int): Number of galaxies to select.\n",
    "        p (callable): PDF function for selection.\n",
    "        p_args (list): Additional arguments for the PDF function.\n",
    "        SFR_Z_type (str): Type of model to use for selection ('old_powerlaw_model', 'old_powerlaw_observed', 'MZR', 'FMR', 'MZSFR').\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Subsampled data.\n",
    "    \"\"\"\n",
    "    if SFR_Z_type=='old_powerlaw_model':\n",
    "        sfr=SFR(np.array(df['z']))\n",
    "        M=np.array(df['M'])\n",
    "        metal=None\n",
    "        prob=p(M,sfr,metal,*p_args)\n",
    "        prob=prob/np.sum(prob)\n",
    "    elif SFR_Z_type=='old_powerlaw_observed':\n",
    "        sfr=np.array(df['sfr'])\n",
    "        metal=np.array(df['metal'])\n",
    "        M=np.array(df['M'])\n",
    "        prob=p(M,sfr,metal,*p_args)\n",
    "        prob=prob/np.sum(prob)\n",
    "    elif SFR_Z_type=='MZR':\n",
    "        M=np.array(df['M'])\n",
    "        prob=p(M,*p_args)\n",
    "        prob=prob/np.sum(prob)\n",
    "    elif SFR_Z_type=='FMR':\n",
    "        sfr=np.array(df['sfr'])\n",
    "        M=np.array(df['M'])\n",
    "        prob=p(M,sfr,*p_args)\n",
    "        prob=prob/np.sum(prob)\n",
    "    elif SFR_Z_type=='MZSFR':\n",
    "        sfr=np.array(df['sfr'])\n",
    "        metal=np.array(df['metal'])\n",
    "        M=np.array(df['M'])\n",
    "        prob=p(M,sfr,metal,*p_args)\n",
    "        prob=prob/np.sum(prob)\n",
    "        \n",
    "    index=np.array(df.index)\n",
    "    new_index=np.random.choice(index, size=(N), replace=False, p=prob)    \n",
    "    new_df=df.loc[new_index]\n",
    "    \n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6d8f41",
   "metadata": {},
   "source": [
    "# Binary Mass Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bcfdea-2b0b-443b-b849-aaff9c363a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_kroupa(m, m_min, m_max, m_power):\n",
    "    \"\"\"\n",
    "    Compute the normalized Kroupa initial mass function (IMF) probability for given masses.\n",
    "\n",
    "    Parameters:\n",
    "        m (float or array-like): Mass or array of masses.\n",
    "        m_min (float): Minimum mass.\n",
    "        m_max (float): Maximum mass.\n",
    "        m_power (float): Power-law index.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Probability values for each mass.\n",
    "    \"\"\"\n",
    "    # Normalization constant for the power-law IMF\n",
    "    norm = (m_max ** (-m_power + 1) - m_min ** (-m_power + 1)) / (-m_power + 1)\n",
    "    p_m = np.array([])\n",
    "    if np.isscalar(m):\n",
    "        m = [m]\n",
    "    # Loop over all masses and compute probability\n",
    "    for i in range(len(m)):\n",
    "        if m[i] < m_min:\n",
    "            p_m = np.append(p_m, 0)\n",
    "        elif m_min <= m[i] <= m_max:\n",
    "            p_m = np.append(p_m, m[i] ** -m_power / norm)\n",
    "        else:\n",
    "            p_m = np.append(p_m, 0)\n",
    "    return p_m\n",
    "\n",
    "def m_kroupa_unnormal(m, m_min, m_max, m_power):\n",
    "    \"\"\"\n",
    "    Compute the unnormalized Kroupa IMF probability for given masses.\n",
    "\n",
    "    Parameters:\n",
    "        m (float or array-like): Mass or array of masses.\n",
    "        m_min (float): Minimum mass.\n",
    "        m_max (float): Maximum mass.\n",
    "        m_power (float): Power-law index.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Unnormalized probability values for each mass.\n",
    "    \"\"\"\n",
    "    p_m = np.array([])\n",
    "    if np.isscalar(m):\n",
    "        m = [m]\n",
    "    # Loop over all masses and compute unnormalized probability\n",
    "    for i in range(np.size(m)):\n",
    "        if m[i] < m_min:\n",
    "            p_m = np.append(p_m, 0)\n",
    "        elif m_min <= m[i] <= m_max:\n",
    "            p_m = np.append(p_m, m[i] ** -m_power)\n",
    "        else:\n",
    "            p_m = np.append(p_m, 0)\n",
    "    return p_m\n",
    "\n",
    "def m_peak_model(m, m_min, m_max, m_power, peak_pos, sigma, height):\n",
    "    \"\"\"\n",
    "    Compute a mass function with a Kroupa IMF plus a Gaussian peak.\n",
    "\n",
    "    Parameters:\n",
    "        m (float or array-like): Mass or array of masses.\n",
    "        m_min (float): Minimum mass.\n",
    "        m_max (float): Maximum mass.\n",
    "        m_power (float): Power-law index.\n",
    "        peak_pos (float): Center of the Gaussian peak.\n",
    "        sigma (float): Width of the Gaussian peak.\n",
    "        height (float): Height of the Gaussian peak.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Normalized probability values for each mass.\n",
    "    \"\"\"\n",
    "    p_m = m_kroupa_unnormal(m, m_min, m_max, m_power) + height * 1 / (sigma * (2 * np.pi) ** 0.5) * np.exp(-0.5 * ((m - peak_pos) / sigma) ** 2)\n",
    "    norm = quad(m_kroupa_unnormal, m_min, m_max, args=(m_min, m_max, m_power))[0] + height\n",
    "    return p_m / norm\n",
    "\n",
    "def sec_mass_sample(mass_sample, m_min):\n",
    "    \"\"\"\n",
    "    Sample secondary masses for binaries uniformly in mass ratio q.\n",
    "\n",
    "    Parameters:\n",
    "        mass_sample (float or array-like): Primary mass or array of primary masses.\n",
    "        m_min (float): Minimum mass for secondary.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of secondary masses.\n",
    "    \"\"\"\n",
    "    if np.isscalar(mass_sample):\n",
    "        mass_sample = [mass_sample]\n",
    "    sec_mass = np.zeros(len(mass_sample))\n",
    "    for i in range(len(mass_sample)):\n",
    "        q = np.random.uniform(m_min / mass_sample[i], 1)\n",
    "        sec_mass[i] = q * mass_sample[i]\n",
    "    return sec_mass\n",
    "\n",
    "def sec_mass_sample_beta(mass_sample, m_min, beta):\n",
    "    \"\"\"\n",
    "    Sample secondary masses for binaries using a beta distribution in mass ratio.\n",
    "\n",
    "    Parameters:\n",
    "        mass_sample (float or array-like): Primary mass or array of primary masses.\n",
    "        m_min (float): Minimum mass for secondary.\n",
    "        beta (float): Power-law index for mass ratio distribution.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of secondary masses.\n",
    "    \"\"\"\n",
    "    if np.isscalar(mass_sample):\n",
    "        mass_sample = [mass_sample]\n",
    "    sec_mass = np.zeros(len(mass_sample))\n",
    "    for i in range(len(mass_sample)):\n",
    "        p_max = find_max(m_kroupa, m_min, mass_sample[i], [m_min, mass_sample[i], -beta])\n",
    "        sec_mass[i] = rejection_sampling(1, m_kroupa, m_min, mass_sample[i], p_max, [m_min, mass_sample[i], -beta])[0]\n",
    "    return sec_mass\n",
    "\n",
    "def find_max(f, x_low, x_high, args):\n",
    "    \"\"\"\n",
    "    Find the maximum value of a function f in the interval [x_low, x_high].\n",
    "\n",
    "    Parameters:\n",
    "        f (callable): Function to maximize.\n",
    "        x_low (float): Lower bound.\n",
    "        x_high (float): Upper bound.\n",
    "        args (list): Arguments for the function f.\n",
    "\n",
    "    Returns:\n",
    "        float: Maximum value of f in the interval.\n",
    "    \"\"\"\n",
    "    x = np.linspace(x_low, x_high, 100)\n",
    "    f_x = f(x, *args)\n",
    "    return max(f_x)\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    \"\"\"\n",
    "    Find the nearest value in an array to a given value.\n",
    "\n",
    "    Parameters:\n",
    "        array (array-like): Array to search.\n",
    "        value (float): Value to find.\n",
    "\n",
    "    Returns:\n",
    "        float: Nearest value in the array.\n",
    "    \"\"\"\n",
    "    idx = np.searchsorted(array, value, side=\"left\")\n",
    "    if idx > 0 and (idx == len(array) or math.fabs(value - array[idx-1]) < math.fabs(value - array[idx])):\n",
    "        return array[idx-1]\n",
    "    else:\n",
    "        return array[idx]\n",
    "\n",
    "def find_nearest_idx(array, value):\n",
    "    \"\"\"\n",
    "    Find the index of the nearest value in an array to a given value.\n",
    "\n",
    "    Parameters:\n",
    "        array (array-like): Array to search.\n",
    "        value (float): Value to find.\n",
    "\n",
    "    Returns:\n",
    "        int: Index of the nearest value in the array.\n",
    "    \"\"\"\n",
    "    idx = np.searchsorted(array, value, side=\"left\")\n",
    "    if idx > 0 and (idx == len(array) or math.fabs(value - array[idx-1]) < math.fabs(value - array[idx])):\n",
    "        return idx-1\n",
    "    else:\n",
    "        return idx\n",
    "\n",
    "def chirp_mass(m1, m2):\n",
    "    \"\"\"\n",
    "    Compute the chirp mass for a binary system.\n",
    "\n",
    "    Parameters:\n",
    "        m1 (float): Mass of the primary.\n",
    "        m2 (float): Mass of the secondary.\n",
    "\n",
    "    Returns:\n",
    "        float: Chirp mass.\n",
    "    \"\"\"\n",
    "    return (m1 * m2) ** (3/5) / (m1 + m2) ** (1/5)\n",
    "\n",
    "def PISN_mass(z, Z_star, M_Z_star, alpha, gamma, zeta):\n",
    "    \"\"\"\n",
    "    Compute the pair-instability supernova (PISN) mass as a function of redshift and metallicity.\n",
    "\n",
    "    Parameters:\n",
    "        z (float): Redshift.\n",
    "        Z_star (float): Reference metallicity.\n",
    "        M_Z_star (float): Reference mass.\n",
    "        alpha, gamma, zeta (float): Model parameters.\n",
    "\n",
    "    Returns:\n",
    "        float: PISN mass.\n",
    "    \"\"\"\n",
    "    Z = 10 ** (gamma * z + zeta)  # metallicity from redshift\n",
    "    return M_Z_star - alpha * np.log10(Z / Z_star)\n",
    "\n",
    "def source_win(z, m, Z_star, M_Z_star, alpha, gamma, zeta):\n",
    "    \"\"\"\n",
    "    Source frame window function for binary formation.\n",
    "\n",
    "    Parameters:\n",
    "        z (float): Redshift.\n",
    "        m (float or array-like): Mass.\n",
    "        Z_star, M_Z_star, alpha, gamma, zeta: Model parameters.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Window function values (1 or 0).\n",
    "    \"\"\"\n",
    "    Z = 10 ** (gamma * z + zeta)\n",
    "    m_PISN = PISN_mass(z, Z_star, M_Z_star, alpha, gamma, zeta)\n",
    "    w = np.piecewise(m, [m < m_PISN, m >= m_PISN], [1, 0])\n",
    "    return w\n",
    "\n",
    "def win_integrand(z, m, Z_star, M_Z_star, alpha, gamma, zeta, z_merg, t_min=100e6, kappa=1):\n",
    "    \"\"\"\n",
    "    Integrand for the window function in the observer frame.\n",
    "\n",
    "    Parameters:\n",
    "        z (float): Redshift.\n",
    "        m (float): Mass.\n",
    "        Z_star, M_Z_star, alpha, gamma, zeta: Model parameters.\n",
    "        z_merg (float): Merger redshift.\n",
    "        t_min (float): Minimum delay time (years).\n",
    "        kappa (float): Power-law index.\n",
    "\n",
    "    Returns:\n",
    "        float: Value of the integrand.\n",
    "    \"\"\"\n",
    "    t_d = cosmo.lookback_time(z).to_value('year') - cosmo.lookback_time(z_merg).to_value('year')\n",
    "    return P_t_d(t_d, kappa=kappa, t_min=t_min) * cosmo.lookback_time_integrand(z) * source_win(z, m, Z_star, M_Z_star, alpha, gamma, zeta)\n",
    "\n",
    "def obs_win(m, z_merg, Z_star, M_Z_star, alpha, gamma, zeta, t_min=100e6, kappa=1):\n",
    "    \"\"\"\n",
    "    Observer frame window function (not normalized).\n",
    "\n",
    "    Parameters:\n",
    "        m (float): Mass.\n",
    "        z_merg (float): Merger redshift.\n",
    "        Z_star, M_Z_star, alpha, gamma, zeta: Model parameters.\n",
    "        t_min (float): Minimum delay time (years).\n",
    "        kappa (float): Power-law index.\n",
    "\n",
    "    Returns:\n",
    "        float: Value of the window function.\n",
    "    \"\"\"\n",
    "    return integrate.quad(win_integrand, z_merg, np.inf, args=(m, Z_star, M_Z_star, alpha, gamma, zeta, z_merg, t_min, kappa), epsabs=1.49e-13, limit=300)[0]\n",
    "\n",
    "def obs_win_normal(m, z_merg, Z_star, M_Z_star, alpha, gamma, zeta, t_min=100e6, kappa=1):\n",
    "    \"\"\"\n",
    "    Normalized observer frame window function.\n",
    "\n",
    "    Parameters:\n",
    "        m (array-like): Array of masses.\n",
    "        z_merg (float): Merger redshift.\n",
    "        Z_star, M_Z_star, alpha, gamma, zeta: Model parameters.\n",
    "        t_min (float): Minimum delay time (years).\n",
    "        kappa (float): Power-law index.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Normalized window function values.\n",
    "    \"\"\"\n",
    "    norm = integrate.quad(obs_win, min(m), max(m), args=(z_merg, Z_star, M_Z_star, alpha, gamma, zeta, t_min, kappa), epsabs=1.49e-13, limit=300)[0]\n",
    "    w = np.zeros(len(m))\n",
    "    for i in range(len(w)):\n",
    "        w[i] = obs_win(m[i], z_merg, Z_star, M_Z_star, alpha, gamma, zeta, t_min, kappa) / norm\n",
    "    return w\n",
    "\n",
    "\n",
    "def binary_masses_sample(df, p_m1_type, p_m1, p_m1_args, p_m2, p_m2_args, z_min, z_max, n_z, t_min=100e6, kappa=1):\n",
    "    \"\"\"\n",
    "    Generate binary component masses (m1, m2) for a galaxy sample using various physical and phenomenological models.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): Input data containing galaxy properties (mass, SFR, metallicity, redshift).\n",
    "        p_m1_type (str): Type of primary mass model ('M_given', 'MSFR_given', 'Z_given', 'phys', 'phen').\n",
    "        p_m1 (callable): PDF/function for primary mass.\n",
    "        p_m1_args (list): Arguments for p_m1.\n",
    "        p_m2 (callable): Function for secondary mass given m1.\n",
    "        p_m2_args (list): Arguments for p_m2.\n",
    "        z_min (float): Minimum redshift for window function grid.\n",
    "        z_max (float): Maximum redshift for window function grid.\n",
    "        n_z (int): Number of redshift grid points for window function.\n",
    "        t_min (float): Minimum delay time (years).\n",
    "        kappa (float): Power-law index for delay time distribution.\n",
    "\n",
    "    Returns:\n",
    "        None. Modifies df in-place by adding 'm1' and 'm2' columns.\n",
    "    \"\"\"\n",
    "    if p_m1_type == 'M_given':\n",
    "        # p_m1_args should be given in this order:\n",
    "        # p_m1_args=[m_min,m_max,m_power,Z_star,M_Z_star,alpha]\n",
    "        # see arXiv:2112.10256v2 for the description of above parameters.\n",
    "        m_min, m_max, m_power, Z_star, M_Z_star, alpha = p_m1_args\n",
    "        num = df.shape[0]\n",
    "        m1 = np.zeros(num)\n",
    "        m2 = np.zeros(num)\n",
    "        M = np.array(df['M'])\n",
    "        solar_OH = 10 ** (8.83 - 12)\n",
    "        solar_metal = 0.017\n",
    "        for i in range(num):\n",
    "            # note that GLADE mass is given in the units of 10^10 solar mass.\n",
    "            OH = 10 ** (8.96 + 0.31 * np.log10(M[i]) - 0.23 * (np.log10(M[i])) ** 2 - 0.017 * (np.log10(M[i])) ** 3 + 0.04 * (np.log10(M[i])) ** 4 - 12)\n",
    "            metal = 10 ** (np.log10(solar_metal) + np.log10(OH) - np.log10(solar_OH))\n",
    "            m_PISN = M_Z_star - alpha * np.log10(metal / Z_star)\n",
    "            p_m1_args_new = [m_min, m_PISN, m_power]\n",
    "            p_max = find_max(p_m1, m_min, m_PISN, p_m1_args_new)\n",
    "            m1[i] = rejection_sampling(1, p_m1, m_min, m_max, p_max, p_m1_args_new)\n",
    "            m2[i] = p_m2(m1[i], *p_m2_args)\n",
    "        df.insert(df.shape[1], 'm1', m1)\n",
    "        df.insert(df.shape[1], 'm2', m2)\n",
    "    if p_m1_type == 'MSFR_given':\n",
    "        # p_m1_args should be given in this order:\n",
    "        # p_m1_args=[m_min,m_max,m_power,Z_star,M_Z_star,alpha]\n",
    "        # see arXiv:2112.10256v2 for the description of above parameters.\n",
    "        m_min, m_max, m_power, Z_star, M_Z_star, alpha = p_m1_args\n",
    "        num = df.shape[0]\n",
    "        m1 = np.zeros(num)\n",
    "        m2 = np.zeros(num)\n",
    "        M = np.array(df['M'])\n",
    "        sfr = np.array(df['sfr'])\n",
    "        solar_OH = 10 ** (8.83 - 12)\n",
    "        solar_metal = 0.017\n",
    "        for i in range(num):\n",
    "            # note that GLADE mass is given in the units of 10^10 solar mass.\n",
    "            OH = 10 ** (8.96 + 0.37 * np.log10(M[i]) - 0.14 * np.log10(sfr[i]) - 0.19 * (np.log10(M[i])) ** 2 + 0.12 * (np.log10(M[i]) * np.log10(sfr[i])) - 0.054 * (np.log10(sfr[i])) ** 2 - 12)\n",
    "            metal = 10 ** (np.log10(solar_metal) + np.log10(OH) - np.log10(solar_OH))\n",
    "            m_PISN = M_Z_star - alpha * np.log10(metal / Z_star)\n",
    "            p_m1_args_new = [m_min, m_PISN, m_power]\n",
    "            p_max = find_max(p_m1, m_min, m_PISN, p_m1_args_new)\n",
    "            m1[i] = rejection_sampling(1, p_m1, m_min, m_max, p_max, p_m1_args_new)\n",
    "            m2[i] = p_m2(m1[i], *p_m2_args)\n",
    "        df.insert(df.shape[1], 'm1', m1)\n",
    "        df.insert(df.shape[1], 'm2', m2)\n",
    "    if p_m1_type == 'Z_given':\n",
    "        # p_m1_args should be given in this order:\n",
    "        # p_m1_args=[m_min,m_max,m_power,Z_star,M_Z_star,alpha]\n",
    "        # see arXiv:2112.10256v2 for the description of above parameters.\n",
    "        m_min, m_max, m_power, Z_star, M_Z_star, alpha = p_m1_args\n",
    "        num = df.shape[0]\n",
    "        m1 = np.zeros(num)\n",
    "        m2 = np.zeros(num)\n",
    "        metal = np.array(df['metal'])\n",
    "        for i in range(num):\n",
    "            m_PISN = M_Z_star - alpha * np.log10(metal[i] / Z_star)\n",
    "            p_m1_args_new = [m_min, m_PISN, m_power]\n",
    "            p_max = find_max(p_m1, m_min, m_PISN, p_m1_args_new)\n",
    "            m1[i] = rejection_sampling(1, p_m1, m_min, m_max, p_max, p_m1_args_new)\n",
    "            m2[i] = p_m2(m1[i], *p_m2_args)\n",
    "        df.insert(df.shape[1], 'm1', m1)\n",
    "        df.insert(df.shape[1], 'm2', m2)\n",
    "    if p_m1_type == 'phys':\n",
    "        # p_m1_args should be given in this order:\n",
    "        # p_m1_args=[m_min,m_max,m_power,Z_star,M_Z_star,alpha,gamma,zeta]\n",
    "        # see arXiv:2112.10256v2 for the description of above parameters.\n",
    "        m_min, m_max, m_power, Z_star, M_Z_star, alpha, gamma, zeta = p_m1_args\n",
    "        m = np.linspace(m_min, m_max, 100)  # an array for generating window functions for m1 to later multiply with source frame mass distribution, 100 resolution could be changed\n",
    "        wins = [1 for _ in range(n_z)]\n",
    "        z_arr = np.linspace(z_min, z_max, n_z)  # redshifts in which we calculate the window function\n",
    "        # generating window functions\n",
    "        for i in range(n_z):\n",
    "            wins[i] = obs_win_normal(m, z_arr[i], Z_star, M_Z_star, alpha, gamma, zeta, t_min=100e6, kappa=1)\n",
    "        num = df.shape[0]\n",
    "        m1 = np.zeros(num)\n",
    "        m2 = np.zeros(num)\n",
    "        z = np.array(df['z'])  # getting actual redshifts of galaxies\n",
    "        p_m1_args_new = [m_min, m_max, m_power]\n",
    "        for i in range(num):\n",
    "            p_s = p_m1(m, *p_m1_args_new)  # source frame mass distribution\n",
    "            idx = find_nearest_idx(z_arr, z[i])  # finding the index of the nearest redshift in z_arr to the galaxy one\n",
    "            p_o = p_s * wins[idx]  # observer frame mass distribution, no need to be normalized\n",
    "            m1[i] = rejection_sampling_numeric(1, m, p_o)\n",
    "            m2[i] = p_m2(m1[i], *p_m2_args)\n",
    "        df.insert(df.shape[1], 'm1', m1)\n",
    "        df.insert(df.shape[1], 'm2', m2)\n",
    "    if p_m1_type == 'phen':\n",
    "        # m_min and m_max should be given as the first and second argument of p_m1_args\n",
    "        m_min = p_m1_args[0]\n",
    "        m_max = p_m1_args[1]\n",
    "        num = df.shape[0]\n",
    "        m1 = np.zeros(num)\n",
    "        m2 = np.zeros(num)\n",
    "        p_max = find_max(p_m1, m_min, m_max, p_m1_args)\n",
    "        for i in range(num):\n",
    "            m1[i] = rejection_sampling(1, p_m1, m_min, m_max, p_max, p_m1_args)\n",
    "            m2[i] = p_m2(m1[i], *p_m2_args)\n",
    "        df.insert(df.shape[1], 'm1', m1)\n",
    "        df.insert(df.shape[1], 'm2', m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b54d1ec-2a19-4d33-a746-b513d5c04863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(cat, thresh=1, nside_mask=256, nside_map=512):\n",
    "    \"\"\"\n",
    "    Generates a binary mask Healpix map given a catalog and threshold.\n",
    "    This is used to mask out regions of the sky with too few sources, which can bias statistical analyses.\n",
    "    \n",
    "    Parameters:\n",
    "        cat (DataFrame): Catalog containing 'ra' and 'dec' columns for right ascension and declination.\n",
    "        thresh (int): Minimum number of sources required in a pixel to keep it in the mask.\n",
    "        nside_mask (int): HEALPix nside parameter for the mask resolution.\n",
    "        nside_map (int): HEALPix nside parameter for the map resolution.\n",
    "    \n",
    "    Returns:\n",
    "        indices_to_mask (np.ndarray): Array of pixel indices at the map resolution that correspond to the mask.\n",
    "        hpx_map (np.ndarray): HEALPix map where pixels with at least 'thresh' sources are set to 1, others are masked out (0).\n",
    "    \"\"\"\n",
    "    # Convert RA/Dec to galactic coordinates\n",
    "    c = SkyCoord(ra=np.asarray(cat.ra) * u.degree, dec=np.asarray(cat.dec) * u.degree, frame='icrs', unit='deg')\n",
    "    l, b = c.galactic.l.value, c.galactic.b.value\n",
    "    theta = np.radians(90. - b)\n",
    "    phi = np.radians(l)\n",
    "    npix = hp.nside2npix(nside_mask)\n",
    "    \n",
    "    # Assign each object to a HEALPix pixel at the mask resolution\n",
    "    indices = hp.ang2pix(nside_mask, theta, phi)\n",
    "    idx, counts = np.unique(indices, return_counts=True)\n",
    "    \n",
    "    # Initialize a HEALPix map at the map resolution\n",
    "    hpx_map = np.zeros(npix)\n",
    "    \n",
    "    # Only keep pixels with at least 'thresh' sources; others are masked out\n",
    "    thresh_idx = idx[counts >= thresh]\n",
    "    hpx_map[thresh_idx] = 1  # 1 = keep, 0 = mask\n",
    "    \n",
    "    # For each object, also get its pixel index at the map resolution (for later masking)\n",
    "    indices_to_mask = hp.ang2pix(nside_map, theta, phi)\n",
    "    \n",
    "    return indices_to_mask, hpx_map\n",
    "\n",
    "def mask_cat(cat, thresh=1, nside_mask=256, nside_map=512):\n",
    "    \"\"\"\n",
    "    Mask a catalog based on a threshold and HEALPix mask.\n",
    "    Intended to remove sources in underpopulated sky regions, but function is incomplete in the original.\n",
    "    \n",
    "    Parameters:\n",
    "        cat (DataFrame): Catalog containing 'ra' and 'dec' columns for right ascension and declination.\n",
    "        thresh (int): Minimum number of sources required in a pixel to keep it in the mask.\n",
    "        nside_mask (int): HEALPix nside parameter for the mask resolution.\n",
    "        nside_map (int): HEALPix nside parameter for the map resolution.\n",
    "        \n",
    "    Returns:\n",
    "        None: Function is incomplete and does not return anything.\n",
    "    \"\"\"\n",
    "    # Convert RA/Dec to galactic coordinates\n",
    "    c = SkyCoord(ra=np.asarray(cat.ra) * u.degree, dec=np.asarray(cat.dec) * u.degree, frame='icrs', unit='deg')\n",
    "    l, b = c.galactic.l.value, c.galactic.b.value\n",
    "    theta = np.radians(90. - b)\n",
    "    phi = np.radians(l)\n",
    "    npix = hp.nside2npix(nside_mask)\n",
    "    pixel_indices = hp.ang2pix(nside_mask, theta, phi)\n",
    "    # The following line references N_z, which is not defined in this function. This may need fixing.\n",
    "    # N_bar = N_z / npix\n",
    "    # print(N_z, npix, N_bar)\n",
    "    # Function is incomplete in the original file. The intent is to mask out sources in low-density pixels.\n",
    "    \n",
    "def N_GW(z_min, z_max, N_bins, obs_time = 1, t_min = 500e6, kappa=1, grid_points=1999):\n",
    "    \"\"\"\n",
    "    Calculate the expected number of gravitational wave events (N_GW) in each redshift bin.\n",
    "    Parameters:\n",
    "        z_min (float): Minimum redshift.\n",
    "        z_max (float): Maximum redshift.\n",
    "        N_bins (int): Number of redshift bins.\n",
    "        obs_time (float): Observation time (default 1 year).\n",
    "        t_min (float): Minimum time delay (default 500 Myr).\n",
    "        kappa (float): Calibration factor (default 1).\n",
    "        grid_points (int): Number of grid points for integration (default 1999).\n",
    "    Returns:\n",
    "        np.ndarray: Array with expected N_GW for each bin.\n",
    "    \"\"\"\n",
    "    bin_size = (z_max - z_min) / N_bins\n",
    "    N_GW_arr = []\n",
    "    for i in range(N_bins):\n",
    "\n",
    "        curr_N_GW = N_GW_bin(bin_size*i+z_min, bin_size*(i+1)+z_min, \\\n",
    "                             obs_time=obs_time, t_min=t_min, kappa=kappa, grid_points=grid_points)\n",
    "        N_GW_arr.append(curr_N_GW)\n",
    "    N_GW_arr = np.asarray(N_GW_arr)\n",
    "    return(N_GW_arr)\n",
    "\n",
    "def sample_sirens(gal_cat_bins,N_GW_arr ,p,p_args, SFR_Z_type,ampl = 1):\n",
    "    '''\n",
    "    Sample gravitational wave sirens from galaxy catalogs.\n",
    "    Parameters:\n",
    "        gal_cat_bins (list): List of galaxy catalogs for each redshift bin.\n",
    "        N_GW_arr (np.ndarray): Expected number of GW events in each bin.\n",
    "        p (function): Probability function for sampling.\n",
    "        p_args (tuple): Arguments for the probability function.\n",
    "        SFR_Z_type (str): Star formation rate and metallicity type.\n",
    "        ampl (list): Amplitude factors for sampling (default [1]).\n",
    "    Returns:\n",
    "        list: Sampled galaxy catalogs for each bin.\n",
    "    '''\n",
    "    df_sampled = []\n",
    "     \n",
    "    for i in range(len(N_GW_arr)):\n",
    "        df_sampled.append(g_selection_full(gal_cat_bins[i],int(N_GW_arr[i] * ampl[i] ),p ,p_args,SFR_Z_type))\n",
    "     \n",
    "    return df_sampled\n",
    "\n",
    "def siren_catalog(df_filtered, z_min, z_max, N_bins, sky_frac,\\\n",
    "                  p, p_args,SFR_Z_type,\\\n",
    "                  p_m1_type,p_m1,p_m1_args, p_m2,p_m2_args,\\\n",
    "                  n_z,\\\n",
    "                  ampl=1, obs_time = 1, \\\n",
    "                  t_min = 500e6, kappa=1, grid_points=1999):\n",
    "    '''\n",
    "    Generate a catalog of gravitational wave sirens.\n",
    "    Parameters:\n",
    "        df_filtered (DataFrame): Filtered galaxy catalog.\n",
    "        z_min (float): Minimum redshift.\n",
    "        z_max (float): Maximum redshift.\n",
    "        N_bins (int): Number of redshift bins.\n",
    "        sky_frac (float): Fraction of the sky covered.\n",
    "        p (function): Probability function for sampling.\n",
    "        p_args (tuple): Arguments for the probability function.\n",
    "        SFR_Z_type (str): Star formation rate and metallicity type.\n",
    "        p_m1_type (str): Mass distribution type for primary mass.\n",
    "        p_m1 (array): Primary mass distribution parameters.\n",
    "        p_m1_args (tuple): Arguments for the primary mass distribution.\n",
    "        p_m2 (array): Secondary mass distribution parameters.\n",
    "        p_m2_args (tuple): Arguments for the secondary mass distribution.\n",
    "        n_z (int): Number of redshift bins for mass distribution.\n",
    "        ampl (list): Amplitude factors for sampling (default [1]).\n",
    "        obs_time (float): Observation time (default 1 year).\n",
    "        t_min (float): Minimum time delay (default 500 Myr).\n",
    "        kappa (float): Calibration factor (default 1).\n",
    "        grid_points (int): Number of grid points for integration (default 1999).\n",
    "    Returns:\n",
    "        tuple: Original and sampled galaxy catalogs for each bin.\n",
    "    '''\n",
    "    df_arr = []\n",
    "    bin_size = (z_max - z_min) / N_bins\n",
    "    \n",
    "    for i in range(N_bins):\n",
    "        df_arr.append(df_filtered[(df_filtered.z >= bin_size*i+z_min) & (df_filtered.z < bin_size*(i+1)+z_min)])\n",
    "        \n",
    "    N_GW_arr = N_GW(z_min, z_max, N_bins, obs_time = obs_time, \\\n",
    "                    t_min = t_min, kappa=kappa, grid_points=grid_points)* sky_frac\n",
    "    \n",
    "    df_sample_arr = sample_sirens(df_arr, N_GW_arr, p, p_args,SFR_Z_type, ampl=ampl)\n",
    "    \n",
    "    for i in range(len(df_sample_arr)):\n",
    "        binary_masses_sample(df_sample_arr[i],p_m1_type,p_m1,p_m1_args,p_m2,p_m2_args,z_min,z_max,n_z,t_min,kappa)\n",
    "\n",
    "    return df_arr, df_sample_arr#, (N_GW_arr * ampl).astype(int)\n",
    "\n",
    "def siren_catalog_r(df,Random, z_min, z_max, N_bins, \n",
    "                  p, p_args,SFR_Z_type,\n",
    "                  p_m1_type,p_m1,p_m1_args, p_m2,p_m2_args,\n",
    "                  n_z,\n",
    "                  ampl=500, obs_time = 1, \n",
    "                  t_min = 500e6, kappa=1, grid_points=1999):\n",
    "    '''\n",
    "    Generate a catalog of gravitational wave sirens with random sampling.\n",
    "    Parameters:\n",
    "        df (DataFrame): Galaxy catalog.\n",
    "        Random (DataFrame): Randomly generated catalog for comparison.\n",
    "        z_min (float): Minimum redshift.\n",
    "        z_max (float): Maximum redshift.\n",
    "        N_bins (int): Number of redshift bins.\n",
    "        p (function): Probability function for sampling.\n",
    "        p_args (tuple): Arguments for the probability function.\n",
    "        SFR_Z_type (str): Star formation rate and metallicity type.\n",
    "        p_m1_type (str): Mass distribution type for primary mass.\n",
    "        p_m1 (array): Primary mass distribution parameters.\n",
    "        p_m1_args (tuple): Arguments for the primary mass distribution.\n",
    "        p_m2 (array): Secondary mass distribution parameters.\n",
    "        p_m2_args (tuple): Arguments for the secondary mass distribution.\n",
    "        n_z (int): Number of redshift bins for mass distribution.\n",
    "        ampl (list): Amplitude factors for sampling (default [500]).\n",
    "        obs_time (float): Observation time (default 1 year).\n",
    "        t_min (float): Minimum time delay (default 500 Myr).\n",
    "        kappa (float): Calibration factor (default 1).\n",
    "        grid_points (int): Number of grid points for integration (default 1999).\n",
    "    Returns:\n",
    "        tuple: Original and sampled galaxy catalogs for each bin, with random samples.\n",
    "    '''\n",
    "    df_arr = []\n",
    "    df_arr_ran=[]\n",
    "    df_sample_ran_arr=[]\n",
    "    bin_size = (z_max - z_min) / N_bins\n",
    "    \n",
    "    for i in range(N_bins):\n",
    "        df_arr.append(df[(df.z >= bin_size*i+z_min) & (df.z < bin_size*(i+1)+z_min)])\n",
    "        df_arr_ran.append(Random[(Random['z'] >= bin_size*i+z_min) & (Random['z'] < bin_size*(i+1)+z_min)])\n",
    "\n",
    "    N_GW_arr = N_GW(z_min, z_max, N_bins, obs_time = obs_time, t_min = t_min, kappa=kappa, grid_points=grid_points)\n",
    "    \n",
    "    df_sample_arr = sample_sirens(df_arr, N_GW_arr, p, p_args,SFR_Z_type, ampl=ampl)\n",
    "    #df_sample_ran_arr = sample_sirens(df_arr_ran, N_GW_arr*10*(ampl), p, p_args,SFR_Z_TYPE, ampl=ampl)\n",
    "\n",
    "    for i in range(N_bins):\n",
    "        df_sample_ran_arr.append((df_arr_ran[i].sample(n=int(N_GW_arr[i]*100*(ampl[i])),replace=False)))\n",
    "\n",
    "    for i in range(len(df_sample_arr)):\n",
    "        binary_masses_sample(df_sample_arr[i],p_m1_type,p_m1,p_m1_args,p_m2,p_m2_args,z_min,z_max,n_z,t_min,kappa)\n",
    "    \n",
    "    return df_arr, df_arr_ran, df_sample_arr,df_sample_ran_arr,(N_GW_arr).astype(int)\n",
    "\n",
    "def rsd_p_gs(M,b_g,b_s,matter_pk,growthrate_s,kedges,sigmap_s):\n",
    "    '''\n",
    "    Calculate the redshift-space distortion power spectrum.\n",
    "    Parameters:\n",
    "        M (array): Mass array.\n",
    "        b_g (array): Galaxy bias array.\n",
    "        b_s (array): Spectroscopic bias array.\n",
    "        matter_pk (array): Matter power spectrum.\n",
    "        growthrate_s (array): Growth rate of structure.\n",
    "        kedges (array): Edges of the k bins.\n",
    "        sigmap_s (array): Sigma for the Gaussian smoothing.\n",
    "    Returns:\n",
    "        array: Redshift-space distortion power spectrum.\n",
    "    '''\n",
    "    return ((b_g*b_s*matter_pk)+(M**2*(b_g+b_s)*growthrate_s*matter_pk)+((M**4)*(growthrate_s**2)*matter_pk))*(1/(1+((kedges**2)*(M**2)*(sigmap_s**2)/2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e45b939-258c-41ee-a599-168937e75dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_wedges(self, muedges, ells=None):\n",
    "    r\"\"\"Transform poles to wedges, with input :math:`\\mu`-edges.\n",
    "    Parameters\n",
    "    ----------\n",
    "    muedges : array\n",
    "        :math:`\\mu`-edges.\n",
    "    ells : tuple, list, default=None\n",
    "        Multipole orders to use in the Legendre expansion.\n",
    "        If ``None``, all poles are used.\n",
    "    Returns\n",
    "    -------\n",
    "    wedges : PowerSpectrumWedges\n",
    "        Power spectrum wedges.\n",
    "    \"\"\"\n",
    "    if ells is None:\n",
    "        ells = self.ells\n",
    "    elif np.ndim(ells) == 0:\n",
    "        ells = [ells]\n",
    "    muedges = np.array(muedges)\n",
    "    mu = (muedges[:-1] + muedges[1:]) / 2.\n",
    "    dmu = np.diff(muedges)\n",
    "    edges = self.edges\n",
    "    modes = (np.repeat(self.modes[0][:, None], len(dmu), axis=-1), np.repeat(mu[None, :], len(self.k), axis=0))\n",
    "    power_nonorm, power_direct_nonorm = 0, 0\n",
    "    for ell in ells:\n",
    "        poly = np.diff(special.legendre(ell).integ()(muedges)) / dmu\n",
    "        power_nonorm += self.power_nonorm[self.ells.index(ell), ..., None] * poly\n",
    "        power_direct_nonorm += self.power_direct_nonorm[self.ells.index(ell), ..., None] * poly\n",
    "    if 0 in self.ells:\n",
    "        power_zero_nonorm = self.power_zero_nonorm[self.ells.index(0)]\n",
    "    else:\n",
    "        power_zero_nonorm = np.array(0., dtype=self.power_zero_nonorm.dtype)\n",
    "    nmodes = self.nmodes[:, None] / dmu\n",
    "    return PowerSpectrumWedges(edges, modes, power_nonorm, nmodes, wnorm=self.wnorm, shotnoise_nonorm=self.shotnoise_nonorm,\n",
    "                               power_zero_nonorm=power_zero_nonorm, power_direct_nonorm=power_direct_nonorm,\n",
    "                               attrs=self.attrs, mpicomm=getattr(self, 'mpicomm', None))\n",
    "def shotnoise(self):\n",
    "        \"\"\"Normalized shot noise.\"\"\"\n",
    "        return self.shotnoise_nonorm / self.wnorm\n",
    "    \n",
    "def N_GW(z_min, z_max, N_bins, obs_time=1, t_min=500e6, kappa=1, grid_points=1999):\n",
    "    '''\n",
    "    Compute the expected number of gravitational wave (GW) events in each redshift bin.\n",
    "    This is used to predict the GW event rate as a function of redshift, which is crucial for population studies and survey planning.\n",
    "    '''\n",
    "    bin_size = (z_max - z_min) / N_bins\n",
    "    N_GW_arr = []\n",
    "    for i in range(N_bins):\n",
    "        # For each bin, integrate the merger rate over the redshift interval\n",
    "        curr_N_GW = N_GW_bin(bin_size * i + z_min, bin_size * (i + 1) + z_min,\n",
    "                             obs_time=obs_time, t_min=t_min, kappa=kappa, grid_points=grid_points)\n",
    "        N_GW_arr.append(curr_N_GW)\n",
    "    N_GW_arr = np.asarray(N_GW_arr)\n",
    "    return N_GW_arr\n",
    "\n",
    "def sample_sirens(gal_cat_bins, N_GW_arr, p, p_args, SFR_Z_type, ampl=1):\n",
    "    '''\n",
    "    Sample gravitational wave sirens from galaxy catalogs in each redshift bin.\n",
    "    This simulates the process of selecting host galaxies for GW events, using a probability model for selection.\n",
    "    '''\n",
    "    df_sampled = []\n",
    "    for i in range(len(N_GW_arr)):\n",
    "        # For each bin, select the expected number of GW hosts using the provided probability model\n",
    "        df_sampled.append(g_selection_full(gal_cat_bins[i], int(N_GW_arr[i] * ampl[i]), p, p_args, SFR_Z_type))\n",
    "    return df_sampled\n",
    "\n",
    "def siren_catalog(df_filtered, z_min, z_max, N_bins, sky_frac,\n",
    "                  p, p_args, SFR_Z_type,\n",
    "                  p_m1_type, p_m1, p_m1_args, p_m2, p_m2_args,\n",
    "                  n_z,\n",
    "                  ampl=1, obs_time=1,\n",
    "                  t_min=500e6, kappa=1, grid_points=1999):\n",
    "    '''\n",
    "    Generate a catalog of gravitational wave sirens (host galaxies) for a survey.\n",
    "    This function bins the galaxy catalog in redshift, computes the expected GW event rate, samples hosts, and assigns binary masses.\n",
    "    '''\n",
    "    df_arr = []\n",
    "    bin_size = (z_max - z_min) / N_bins\n",
    "    # Bin the galaxy catalog by redshift\n",
    "    for i in range(N_bins):\n",
    "        df_arr.append(df_filtered[(df_filtered.z >= bin_size * i + z_min) & (df_filtered.z < bin_size * (i + 1) + z_min)])\n",
    "    # Compute expected GW events per bin, scaled by sky coverage\n",
    "    N_GW_arr = N_GW(z_min, z_max, N_bins, obs_time=obs_time,\n",
    "                    t_min=t_min, kappa=kappa, grid_points=grid_points) * sky_frac\n",
    "    # Sample host galaxies for each bin\n",
    "    df_sample_arr = sample_sirens(df_arr, N_GW_arr, p, p_args, SFR_Z_type, ampl=ampl)\n",
    "    # For each sampled host, assign binary component masses\n",
    "    for i in range(len(df_sample_arr)):\n",
    "        binary_masses_sample(df_sample_arr[i], p_m1_type, p_m1, p_m1_args, p_m2, p_m2_args, z_min, z_max, n_z, t_min, kappa)\n",
    "    return df_arr, df_sample_arr  # Also returns the original binned catalogs\n",
    "\n",
    "def siren_catalog_r(df, Random, z_min, z_max, N_bins,\n",
    "                  p, p_args, SFR_Z_type,\n",
    "                  p_m1_type, p_m1, p_m1_args, p_m2, p_m2_args,\n",
    "                  n_z,\n",
    "                  ampl=500, obs_time=1,\n",
    "                  t_min=500e6, kappa=1, grid_points=1999):\n",
    "    '''\n",
    "    Generate a catalog of GW sirens with random sampling for comparison.\n",
    "    This is useful for null tests and for understanding the impact of selection effects.\n",
    "    '''\n",
    "    df_arr = []\n",
    "    df_arr_ran = []\n",
    "    df_sample_ran_arr = []\n",
    "    bin_size = (z_max - z_min) / N_bins\n",
    "    # Bin both the real and random catalogs by redshift\n",
    "    for i in range(N_bins):\n",
    "        df_arr.append(df[(df.z >= bin_size * i + z_min) & (df.z < bin_size * (i + 1) + z_min)])\n",
    "        df_arr_ran.append(Random[(Random['z'] >= bin_size * i + z_min) & (Random['z'] < bin_size * (i + 1) + z_min)])\n",
    "    # Compute expected GW events per bin\n",
    "    N_GW_arr = N_GW(z_min, z_max, N_bins, obs_time=obs_time,\n",
    "                    t_min=t_min, kappa=kappa, grid_points=grid_points)\n",
    "    # Sample hosts from the real catalog\n",
    "    df_sample_arr = sample_sirens(df_arr, N_GW_arr, p, p_args, SFR_Z_type, ampl=ampl)\n",
    "    # Sample random hosts for null test (oversample by 100x for statistics)\n",
    "    for i in range(N_bins):\n",
    "        df_sample_ran_arr.append((df_arr_ran[i].sample(n=int(N_GW_arr[i] * 100 * (ampl[i])), replace=False)))\n",
    "    # Assign binary masses to the real sampled hosts\n",
    "    for i in range(len(df_sample_arr)):\n",
    "        binary_masses_sample(df_sample_arr[i], p_m1_type, p_m1, p_m1_args, p_m2, p_m2_args, z_min, z_max, n_z, t_min, kappa)\n",
    "    return df_arr, df_arr_ran, df_sample_arr, df_sample_ran_arr, (N_GW_arr).astype(int)\n",
    "\n",
    "def rsd_p_gs(M, b_g, b_s, matter_pk, growthrate_s, kedges, sigmap_s):\n",
    "    '''\n",
    "    Calculate the redshift-space distortion (RSD) power spectrum for a galaxy sample.\n",
    "    This is used to model the observed clustering of galaxies, accounting for peculiar velocities.\n",
    "    '''\n",
    "    # The formula combines galaxy bias, matter power spectrum, and growth rate, with Gaussian smoothing for small scales\n",
    "    return ((b_g * b_s * matter_pk) + (M ** 2 * (b_g + b_s) * growthrate_s * matter_pk) + ((M ** 4) * (growthrate_s ** 2) * matter_pk)) * (1 / (1 + ((kedges ** 2) * (M ** 2) * (sigmap_s ** 2) / 2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21092a24-8535-40e4-a55d-8efcb142d077",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af54456b-bb8e-415f-b2a3-51b86283244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility of all random operations in this notebook.\n",
    "np.random.seed(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-night",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the random catalog, main galaxy data, and k-correction data from FITS files.\n",
    "# These files contain the SDSS random points, galaxy sample, and k-corrections for absolute magnitudes.\n",
    "# The random catalog is used for statistical corrections and completeness estimation.\n",
    "# The main data catalog contains the observed galaxies.\n",
    "# The k-correct file is used to correct observed magnitudes to a common rest-frame.\n",
    "# Note: File paths are specific to the data storage system in use.\n",
    "#random=fits.open('/gpfs/dsadathosseini/sdss_random/random-0.dr72safe.fits')[1].data\n",
    "random=fits.open('/gpfs/dsadathosseini/LSS2/sdss.physics.nyu.edu/lss/dr72/safe/Random_final/random/concatenated.fits')[1].data\n",
    "data=fits.open('/gpfs/dsadathosseini/LSS2/sdss.physics.nyu.edu/lss/dr72/safe/0/post_catalog.dr72safe0.fits')[1].data\n",
    "k_correct=fits.open('kcorrect.nearest.petro.z0.10.fits')[1].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c65a1a-c0e9-446b-b5dc-54906a1c6df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following block (commented out) shows how to concatenate multiple FITS files into a single table.\n",
    "# This is useful if the random catalog is split across many files and needs to be merged for analysis.\n",
    "# It uses astropy's Table and vstack to combine all FITS tables in a directory tree.\n",
    "# Uncomment and run if you need to regenerate the concatenated random catalog.\n",
    "\n",
    "# from astropy.table import Table, vstack\n",
    "# import os\n",
    "\n",
    "# # Define the directory where the FITS files are located\n",
    "# fits_dir = '/gpfs/dsadathosseini/LSS2/sdss.physics.nyu.edu/lss/dr72/safe/Random_final/random'\n",
    "\n",
    "# # Initialize an empty table to hold the concatenated data\n",
    "# concatenated_table = Table()\n",
    "\n",
    "# # Loop over all files in the directory (and its subdirectories)\n",
    "# for root, dirs, files in os.walk(fits_dir):\n",
    "#     for file in files:\n",
    "#         # Check if the file is a FITS file\n",
    "#         if file.endswith('.fits'):\n",
    "#             # Construct the full path to the file\n",
    "#             file_path = os.path.join(root, file)\n",
    "#             # Read the data from the FITS file into a table\n",
    "#             print(file_path)\n",
    "#             table = Table.read(file_path)\n",
    "#             # Append the table to the concatenated table\n",
    "#             concatenated_table = vstack([concatenated_table, table])\n",
    "\n",
    "# # Write the concatenated table to a new FITS file\n",
    "# concatenated_table.write('/gpfs/dsadathosseini/LSS2/sdss.physics.nyu.edu/lss/dr72/safe/Random_final/random/concatenated.fits', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-surgery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the sky completeness map using the random catalog.\n",
    "# This is used to correct for survey geometry and selection effects.\n",
    "countsmap_random = countmap(np.array(random['RA']), np.array(random['DEC']), nside=256)\n",
    "wheightedmap_random = weighted_countmap(np.array(random['RA']), np.array(random['DEC']), np.array(random['FGOT']), nside=256)\n",
    "ave = wheightedmap_random / countsmap_random  # Average completeness per pixel\n",
    "\n",
    "# Assign completeness to each galaxy based on its sky position\n",
    "# This allows us to later weight galaxies by their local completeness\n",
    "\n",
    "gal_indices = indices(np.array(data['RA']), np.array(data['DEC']), nside=256)\n",
    "fgot_galaxy = ave[gal_indices]\n",
    "\n",
    "# Compute galactic coordinates for each galaxy and random point\n",
    "# These are used for spatial selection and for mapping the data on the sky\n",
    "c = SkyCoord(np.array(data['RA']), np.array(data['DEC']), unit=(u.degree, u.degree))\n",
    "latitude = c.galactic.b.value\n",
    "longtitude = c.galactic.l.value\n",
    "\n",
    "c1 = SkyCoord(np.array(random['RA']), np.array(random['DEC']), unit=(u.degree, u.degree))\n",
    "latitude_r = c1.galactic.b.value\n",
    "longtitude_r = c1.galactic.l.value\n",
    "\n",
    "# Add new columns to the data tables for galactic coordinates and completeness\n",
    "# This makes it easier to apply spatial cuts and weights later\n",
    "\n",
    "data.Latitude = latitude\n",
    "data.Longtitude = longtitude\n",
    "data.FGOT_GALAXY = fgot_galaxy\n",
    "random.Latitude = latitude_r\n",
    "random.Longtitude = longtitude_r\n",
    "\n",
    "# Compute color indices for the galaxy sample (g, r, and g-r)\n",
    "# These are used for further selection and for stellar population studies\n",
    "g_mag = np.array(data['ABSM'])[:, 1]\n",
    "r_mag = np.array(data['ABSM'])[:, 2]\n",
    "gr_mag = np.array(data['ABSM'])[:, 1] - np.array(data['ABSM'])[:, 2]\n",
    "\n",
    "\n",
    "# Apply selection cuts to define the final galaxy and random samples\n",
    "# These cuts remove galaxies with low completeness, outside the desired redshift range,\n",
    "# and in problematic regions of the sky (e.g., near the Galactic plane or survey edges)\n",
    "Data = data[(data['Z'] > 0.07) & (data['Z'] < 0.2) & (data.FGOT_GALAXY > 0.9) & (0 < data.Latitude) & ~((data.Latitude > 31) & (45 > data.Latitude) & (data.Longtitude > 75) & (97 > data.Longtitude))]\n",
    "Random = random[(random['FGOT'] > 0.9) & (0 < random.Latitude) & ~((random.Latitude > 31) & (45 > random.Latitude) & (random.Longtitude > 75) & (97 > random.Longtitude))]\n",
    "\n",
    "# Compute FKP weights for the galaxy sample\n",
    "# These weights optimize the power spectrum measurement by downweighting regions with high density\n",
    "P_FKP = 16000\n",
    "Z_d = np.array(Data['Z'])\n",
    "NZ_d = np.zeros(len(Z_d))\n",
    "weight_FKP_d = np.zeros(len(Z_d))\n",
    "for i in range(len(Z_d)):\n",
    "    if ((Z_d[i] >= 0.17) & (Z_d[i] < 0.2)):\n",
    "        n_z = 0.00286 - 0.0131 * Z_d[i]\n",
    "        NZ_d[i] = n_z\n",
    "        w_fkp = 1 / (1 + (P_FKP * n_z))\n",
    "        weight_FKP_d[i] = w_fkp\n",
    "    elif ((Z_d[i] > 0.07) & (Z_d[i] < 0.17)):\n",
    "        n_z = 0.0014 * Z_d[i] + 0.00041\n",
    "        NZ_d[i] = n_z\n",
    "        w_fkp = 1 / (1 + (P_FKP * n_z))\n",
    "        weight_FKP_d[i] = w_fkp\n",
    "Data.NZ = NZ_d\n",
    "Data.W_FKP = weight_FKP_d\n",
    "\n",
    "# Assign random redshifts to the random catalog, drawn from the galaxy redshift distribution\n",
    "# This is necessary for statistical corrections in clustering measurements\n",
    "safe_z = Data['Z']\n",
    "random_z = np.random.choice(safe_z, 33522980, replace=True)\n",
    "Random.z = random_z\n",
    "\n",
    "# Compute FKP weights for the random catalog using the same formula\n",
    "P_FKP = 16000\n",
    "Z_r = np.array(Random.z)\n",
    "NZ_r = np.zeros(len(Z_r))\n",
    "weight_FKP_r = np.zeros(len(Z_r))\n",
    "for i in range(len(Z_r)):\n",
    "    if ((Z_r[i] >= 0.17) & (Z_r[i] < 0.2)):\n",
    "        n_z = 0.00286 - 0.0131 * Z_r[i]\n",
    "        NZ_r[i] = n_z\n",
    "        w_fkp = 1 / (1 + (P_FKP * n_z))\n",
    "        weight_FKP_r[i] = w_fkp\n",
    "    elif ((Z_r[i] > 0.07) & (Z_r[i] < 0.17)):\n",
    "        n_z = 0.0014 * Z_r[i] + 0.00041\n",
    "        NZ_r[i] = n_z\n",
    "        w_fkp = 1 / (1 + (P_FKP * n_z))\n",
    "        weight_FKP_r[i] = w_fkp\n",
    "Random.NZ = NZ_r\n",
    "Random.W_FKP = weight_FKP_r\n",
    "\n",
    "# Assign weights to the galaxy and random samples\n",
    "# For galaxies, use the inverse of the completeness as the weight\n",
    "Data.WEIGHT = np.ones(335818)\n",
    "Data.WEIGHT = 1. / data.FGOT_GALAXY\n",
    "Random.WEIGHT = np.ones(33522980)\n",
    "\n",
    "# Compute k-corrected magnitudes and colors for the galaxy sample\n",
    "# These are used for stellar population and color-magnitude analyses\n",
    "k_g_mag = np.array(k_correct['ABSMAG'])[:, 1]\n",
    "k_r_mag = np.array(k_correct['ABSMAG'])[:, 2]\n",
    "k_gr_mag = np.array(k_correct['ABSMAG'])[:, 1] - np.array(k_correct['ABSMAG'])[:, 2]\n",
    "\n",
    "# Assign completeness and galactic coordinates to the k-corrected sample\n",
    "k_gal_indices = indices(np.array(k_correct['RA']), np.array(k_correct['DEC']), nside=256)\n",
    "fgot_k_galaxy = ave[k_gal_indices]\n",
    "ck = SkyCoord(np.array(k_correct['RA']), np.array(k_correct['DEC']), unit=(u.degree, u.degree))\n",
    "latitude_k = ck.galactic.b.value\n",
    "longtitude_k = ck.galactic.l.value\n",
    "k_correct.Latitude = latitude_k \n",
    "k_correct.Longtitude = longtitude_k\n",
    "k_correct.FGOT_GALAXY = fgot_k_galaxy \n",
    "\n",
    "K_correct = k_correct[(k_correct['Z'] > 0.07) & (k_correct['Z'] < 0.2) & (k_correct.FGOT_GALAXY > 0.9) & (0 < k_correct.Latitude) & ~((k_correct.Latitude > 31) & (45 > k_correct.Latitude) & (k_correct.Longtitude > 75) & (97 > k_correct.Longtitude))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-terrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a table for the random catalog with relevant columns for clustering analysis.\n",
    "# This table is converted to a pandas DataFrame for easier manipulation and compatibility with later analysis steps.\n",
    "# The random catalog is used to model the survey selection function and correct for observational biases.\n",
    "Random_a = Table([\n",
    "    np.array(Random.RA),\n",
    "    np.array(Random.DEC),\n",
    "    np.array(Random.W_FKP),\n",
    "    np.array(Random.MMAX),\n",
    "    np.array(Random.NZ),\n",
    "    np.array(Random.z),\n",
    "    np.array(Random.WEIGHT)\n",
    "], names=('RA', 'DEC', 'W_FKP', 'MMAX', 'NZ', 'z', 'Weight'))\n",
    "Random_ap = Random_a.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "setup_logging()  # Turn on logging to screen for nbodykit and related libraries\n",
    "\n",
    "# Set up the fiducial BOSS DR12 cosmology for all distance and clustering calculations\n",
    "cosmolo = cosmology.Cosmology(h=0.7).match(Omega0_m=0.31)\n",
    "\n",
    "# Add Cartesian position columns to the galaxy and random catalogs\n",
    "# This is required for 3D clustering and power spectrum analysis\n",
    "Data.Position = transform.SkyToCartesian(Data['RA'], Data['DEC'], Data['Z'], cosmo=cosmolo)\n",
    "Random.Position = transform.SkyToCartesian(Random['RA'], Random['DEC'], Random.z, cosmo=cosmolo)\n",
    "\n",
    "# Match each galaxy in the main sample to the nearest k-corrected galaxy in the sky\n",
    "# This is used to assign stellar population properties (SFR, metallicity, mass) to each galaxy\n",
    "cd = SkyCoord(ra=np.array(Data['RA']) * u.degree, dec=np.array(Data['DEC']) * u.degree)\n",
    "ck = SkyCoord(ra=np.array(K_correct['RA']) * u.degree, dec=np.array(K_correct['DEC']) * u.degree)\n",
    "idx, d2d, d3d = cd.match_to_catalog_sky(ck)\n",
    "\n",
    "# Build a new table with matched properties for each galaxy\n",
    "# SFR is computed from INTSFH and B1000, metallicity from METS, mass from MASS, and redshift from Z\n",
    "# NZ and W_FKP are carried over for completeness and weighting\n",
    "# The resulting DataFrame is the main input for further analysis\n",
    "\n",
    "t1 = Table([\n",
    "    np.array(K_correct[idx]['RA']),\n",
    "    np.array(K_correct[idx]['DEC']),\n",
    "    ((np.array(K_correct[idx]['INTSFH'])) * (np.array(K_correct[idx]['B1000']))) / (((0.67) ** 2) * 10 ** 9),\n",
    "    np.array(K_correct[idx]['METS']),\n",
    "    ((np.array(K_correct[idx]['MASS'])) * (1 / (10 ** (10) * (0.67) ** 2))),\n",
    "    np.array(K_correct[idx]['Z']),\n",
    "    np.array(Data.NZ),\n",
    "    np.array(Data.W_FKP)\n",
    "], names=('ra', 'dec', 'sfr', 'metal', 'M', 'z', 'NZ', 'W_FKP'))\n",
    "df = t1.to_pandas()\n",
    "\n",
    "# Filter the galaxy catalog to remove any entries with missing values\n",
    "# This ensures that all subsequent analysis is performed on clean, complete data\n",
    "df_filtered = df[df.z.notnull() & df.M.notnull() & df.ra.notnull() & df.dec.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e268cf0a-14da-4971-a1c4-e79159e62128",
   "metadata": {},
   "source": [
    "# Generating siren catalog using MZR host - galaxy probablity function / For examining GW bias dependency on Mk(pivot point in the probability function) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-wedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block generates the final siren catalogs and performs the main analysis loop for a range of M_c (pivot mass) values.\n",
    "# The goal is to study how the GW host galaxy bias depends on the mass-metallicity relation (MZR) pivot point.\n",
    "\n",
    "# Define the range of M_c values (pivot points for the probability function)\n",
    "M_c_values = np.logspace(-1, 2, 15)\n",
    "for i, M_c in enumerate(M_c_values):\n",
    "    np.random.seed(i)  # Set seed for reproducibility for each M_c\n",
    "    data_g = Data\n",
    "    random_g = Random_ap\n",
    "    K_correct_g = K_correct\n",
    "    z_min = 0.07\n",
    "    z_max = 0.20\n",
    "    N_bins = 2  # Number of redshift bins\n",
    "    m_bins = 1  # Number of mass bins (can be scalar or list of bin edges)\n",
    "    p = P_MZR  # Probability function for host selection\n",
    "    p_args = [4, .5, M_c, 0, 150]  # Arguments for the probability function\n",
    "    SFR_Z_type = 'MZR'  # Use the MZR for host selection\n",
    "    p_m1_type = 'Z_given'  # Use metallicity for primary mass selection\n",
    "    p_m1 = m_kroupa  # Primary mass function\n",
    "    p_m1_args = [5, 50, 2.3, 1e-4, 45, 1.5]  # Arguments for primary mass function\n",
    "    p_m2 = sec_mass_sample_beta  # Secondary mass function\n",
    "    p_m2_args = [5, 1]  # Arguments for secondary mass function\n",
    "    n_z = 10  # Number of redshift grid points for window function\n",
    "    ampl = 2000  # Amplitude factor for sampling\n",
    "    obs_time = 1  # Observation time (years)\n",
    "    t_min = 500e6  # Minimum time delay (years)\n",
    "    kappa = 1  # Power-law index for delay time distribution\n",
    "    grid_points = 1999  # Number of grid points for integration\n",
    "    thresh = 15  # Threshold for masking\n",
    "    nside_map = 512\n",
    "    nside_mask = 256\n",
    "\n",
    "    # Generate the siren and random catalogs for this M_c value\n",
    "    # This uses the full selection, mass modeling, and sampling pipeline\n",
    "    gal_cat_arr_k, gal_cat_ran_arr_k, siren_cat_arr_k, siren_cat_ran_arr_k, GW_Num_k = siren_catalog_r(\n",
    "        df_filtered, Random_ap, z_min, z_max, N_bins,\n",
    "        p, p_args, SFR_Z_type,\n",
    "        p_m1_type, p_m1, p_m1_args, p_m2, p_m2_args,\n",
    "        n_z,\n",
    "        ampl=ampl, obs_time=obs_time,\n",
    "        t_min=t_min, kappa=kappa, grid_points=grid_points)\n",
    "\n",
    "    # Save the generated catalogs for later analysis\n",
    "    Catalogs_M_c = siren_cat_arr_k, siren_cat_ran_arr_k\n",
    "    np.save('/gpfs/dsadathosseini/sdss_MZR_s50_dl4_dh05_newomcorr_file_final/Catalogs_M_c' + str(M_c), Catalogs_M_c)\n",
    "\n",
    "    # Mass modeling: compute the chirp mass for each binary in the siren catalog\n",
    "    for i in range(len(siren_cat_arr_k)):\n",
    "        siren_cat_arr_k[i]['chirp_mass'] = chirp_mass(np.array(siren_cat_arr_k[i]['m1']), np.array(siren_cat_arr_k[i]['m2']))\n",
    "\n",
    "    # Bin the siren catalog by chirp mass if needed\n",
    "    if np.isscalar(m_bins):\n",
    "        m2_min, m1_min, m_max = p_m2_args[0], p_m1_args[0], p_m1_args[1]\n",
    "        cm_min, cm_max = chirp_mass(m2_min, m1_min), chirp_mass(m_max, m_max)\n",
    "        m_bin_size = (cm_max - cm_min) / m_bins\n",
    "        df_arr_k = [[1 for j in range(m_bins)] for i in range(N_bins)]\n",
    "        for i in range(len(siren_cat_arr_k)):\n",
    "            for j in range(m_bins):\n",
    "                df_arr_k[i][j] = (siren_cat_arr_k[i][(siren_cat_arr_k[i].chirp_mass >= m_bin_size * j + cm_min)\n",
    "                                                    & (siren_cat_arr_k[i].chirp_mass < m_bin_size * (j + 1) + cm_min)])\n",
    "        siren_cat_arr_k = df_arr_k  # Overwrite with binned catalog\n",
    "    else:\n",
    "        # m_bins is a list of bin edges\n",
    "        df_arr_k = [[1 for j in range(len(m_bins))] for i in range(N_bins)]\n",
    "        for i in range(len(siren_cat_arr_k)):\n",
    "            for j in range(len(m_bins)):\n",
    "                df_arr_k[i][j] = (siren_cat_arr_k[i][(siren_cat_arr_k[i].chirp_mass >= m_bins[j][0])\n",
    "                                                    & (siren_cat_arr_k[i].chirp_mass < m_bins[j][1])])\n",
    "        siren_cat_arr_k = df_arr_k\n",
    "    if np.isscalar(m_bins) == False:\n",
    "        m_bins = len(m_bins)\n",
    "\n",
    "    # Prepare catalogs for nbodykit power spectrum analysis\n",
    "    # Convert pandas DataFrames to astropy Tables and then to ArrayCatalogs\n",
    "    gal_cat_arr_cat_k = list(map(lambda x: Table.from_pandas(x), gal_cat_arr_k))\n",
    "    gal_cat_ran_arr_cat_k = list(map(lambda x: Table.from_pandas(x), gal_cat_ran_arr_k))\n",
    "    gal_cat_arr_cat1_k = list(map(lambda x: ArrayCatalog(x), gal_cat_arr_cat_k))\n",
    "    gal_cat_ran_arr_cat1_k = list(map(lambda x: ArrayCatalog(x), gal_cat_ran_arr_cat_k))\n",
    "    siren_cat_arr_cat_k = []\n",
    "    siren_cat_ran_arr_cat_k = []\n",
    "    siren_cat_arr_cat1_k = []\n",
    "    siren_cat_ran_arr_cat1_k = []\n",
    "    for i in range(len(siren_cat_arr_k)):\n",
    "        siren_cat_arr_cat_k.append(list(map(lambda x: Table.from_pandas(x), siren_cat_arr_k[i])))\n",
    "    for i in range(len(siren_cat_ran_arr_k)):\n",
    "        siren_cat_ran_arr_cat_k.append(list(map(lambda x: Table.from_pandas(x), siren_cat_ran_arr2_k[i])))\n",
    "    for i in range(len(siren_cat_arr_cat_k)):\n",
    "        siren_cat_arr_cat1_k.append(list(map(lambda x: ArrayCatalog(x), siren_cat_arr_cat_k[i])))\n",
    "    for i in range(len(siren_cat_ran_arr_cat_k)):\n",
    "        siren_cat_ran_arr_cat1_k.append(list(map(lambda x: ArrayCatalog(x), siren_cat_ran_arr_cat_k[i])))\n",
    "\n",
    "    # Assign 3D positions to all catalogs using the fiducial cosmology\n",
    "    cosmolo = cosmology.Cosmology(h=0.7).match(Omega0_m=0.31)\n",
    "    for i in range(len(gal_cat_arr_cat1_k)):\n",
    "        gal_cat_arr_cat1_k[i]['Position'] = transform.SkyToCartesian(gal_cat_arr_cat1_k[i]['ra'], gal_cat_arr_cat1_k[i]['dec'], gal_cat_arr_cat1_k[i]['z'], cosmo=cosmolo)\n",
    "    for i in range(len(gal_cat_ran_arr_cat1_k)):\n",
    "        gal_cat_ran_arr_cat1_k[i]['Position'] = transform.SkyToCartesian(gal_cat_ran_arr_cat1_k[i]['RA'], gal_cat_ran_arr_cat1_k[i]['DEC'], gal_cat_ran_arr_cat1_k[i]['z'], cosmo=cosmolo)\n",
    "    for i in range(N_bins):\n",
    "        for j in range(m_bins):\n",
    "            siren_cat_arr_cat1_k[i][j]['Position'] = transform.SkyToCartesian(siren_cat_arr_cat1_k[i][j]['ra'], siren_cat_arr_cat1_k[i][j]['dec'], siren_cat_arr_cat1_k[i][j]['z'], cosmo=cosmolo)\n",
    "    for i in range(N_bins):\n",
    "        for j in range(m_bins):\n",
    "            siren_cat_ran_arr_cat1_k[i][j]['Position'] = transform.SkyToCartesian(siren_cat_ran_arr_cat1_k[i][j]['RA'], siren_cat_ran_arr_cat1_k[i][j]['DEC'], siren_cat_ran_arr_cat1_k[i][j]['z'], cosmo=cosmolo)\n",
    "\n",
    "    #power of galaxies\n",
    "\n",
    "    P0 = 1e4\n",
    "    kmin=0.0\n",
    "    kmax=0.5\n",
    "    dk=0.02\n",
    "    kbin=int((kmax-kmin)/dk)\n",
    "    kedges = np.linspace(kmin, kmax, kbin+1)\n",
    "    gal_pk_pypower=np.zeros((N_bins, kbin))\n",
    "    gal_pk_pypower_sn = np.zeros((N_bins))\n",
    "    matter_g_pk_pypower=np.zeros((N_bins,kbin))\n",
    "    FKP_gal_pypower=np.zeros(N_bins)\n",
    "    C_g_k=np.zeros((N_bins,kbin))\n",
    "    sigma_p_g=np.zeros((N_bins))\n",
    "    f_g=np.zeros((N_bins))\n",
    "    data=gal_cat_arr_cat1_k\n",
    "    randoms=gal_cat_ran_arr_cat1_k\n",
    "    P_g_rsd=np.zeros((N_bins))\n",
    "\n",
    "    # Loop over redshift bins to compute galaxy power spectrum and related quantities\n",
    "    for i in range(N_bins):\n",
    "        result_G = {}\n",
    "        zbin_names = ['zbin1', 'zbin2']\n",
    "\n",
    "        ells = (0, 2, 4)\n",
    "        data=gal_cat_arr_cat1_k\n",
    "        randoms=gal_cat_ran_arr_cat1_k\n",
    "        # Define redshift interval and box size for each bin (survey geometry)\n",
    "        if i==0 :\n",
    "            interval=np.linspace(0.07, 0.135, 100)\n",
    "            BoxSize = [272., 535., 300.]\n",
    "            Boxcenter = [-159.74, -11.47, 127.62]\n",
    "        elif i==1: \n",
    "            interval=np.linspace(0.135, 0.2, 100)\n",
    "            BoxSize = [544., 1040., 585.]\n",
    "            Boxcenter = [-304.22, -21.37, 247.3] \n",
    "\n",
    "\n",
    "        FSKY = 6813./41253. # a made-up value\n",
    "        # Compute redshift histogram for randoms to estimate selection function\n",
    "        zhist_g_i_k = RedshiftHistogram(gal_cat_ran_arr_cat1_k[i], FSKY, cosmolo, redshift='z',bins = interval)\n",
    "        alpha_g_i_k = 1.0 * gal_cat_arr_cat1_k[i].csize / gal_cat_ran_arr_cat1_k[i].csize\n",
    "        # Interpolate n(z) for data and randoms\n",
    "        nz_g_i = np.interp(np.array( gal_cat_arr_cat1_k[i]['z']),zhist_g_i_k.bin_centers,alpha_g_i_k*zhist_g_i_k.nbar)\n",
    "        nz_gr_i = np.interp(np.array(gal_cat_ran_arr_cat1_k[i]['z']),zhist_g_i_k.bin_centers,alpha_g_i_k*zhist_g_i_k.nbar)\n",
    "        # Build FKP catalog with weights for optimal power spectrum estimation\n",
    "        fkp_gal_i = FKPCatalog(gal_cat_arr_cat1_k[i], gal_cat_ran_arr_cat1_k[i])\n",
    "        fkp_gal_i['randoms/NZ'] = nz_gr_i\n",
    "        fkp_gal_i['data/NZ'] = nz_g_i\n",
    "        fkp_gal_i['data/FKPWeight'] = 1.0 / (1 + fkp_gal_i['data/NZ'] * P0 )\n",
    "        fkp_gal_i['randoms/FKPWeight'] = 1.0 / (1 + fkp_gal_i['randoms/NZ'] * P0 )\n",
    "        # Compute weighted mean redshift for the bin\n",
    "        FKP_gal_pypower[i]=np.sum(np.array(fkp_gal_i['data/FKPWeight']) * np.array(gal_cat_arr_cat1_k[i]['z']))/np.sum(np.array(fkp_gal_i['data/FKPWeight']))\n",
    "\n",
    "        # Compute the power spectrum using FFT-based estimator\n",
    "        result_g_i = CatalogFFTPower(data_positions1=np.array(gal_cat_arr_cat1_k[i]['Position']), data_weights1=np.array(fkp_gal_i['data/FKPWeight']),\n",
    "                                 randoms_positions1=np.array(gal_cat_ran_arr_cat1_k[i]['Position']), randoms_weights1=np.array(fkp_gal_i['randoms/FKPWeight']),\n",
    "                                 edges=kedges, ells=ells, interlacing=2, boxsize=None,boxcenter=None, nmesh=256, resampler='tsc',\n",
    "                                 los='firstpoint', position_type='pos', mpiroot=0)\n",
    "        # Save the result for later analysis\n",
    "        result_g_i.save('/gpfs/dsadathosseini/sdss_mzsfr_newomcorr_deltah'+str(deltah)+'_Z002_sfr372_eph05_zetal05_zetah025_file_final/result_g_Mc'+str(M_c)+'_i'+str(i)) \n",
    "\n",
    "\n",
    "        Nmu = 1\n",
    "        mu_edges = numpy.linspace(0, 1, Nmu+1)\n",
    "        poles_g_i = result_g_i.poles\n",
    "        wedges_g_i = poles_g_i.to_wedges(muedges=mu_edges, ells=None)\n",
    "        gal_pk_pypower[i,:]=wedges_g_i.power[:,0]\n",
    "\n",
    "\n",
    "\n",
    "        gal_pk_pypower_sn[i]=shotnoise(wedges_g_i)\n",
    "        c =cosmology.Planck15\n",
    "        sigma_v=400\n",
    "        zeff_g_i=np.sum(np.array(gal_cat_arr_cat1_k[i]['z'] * np.array(fkp_gal_i['data/FKPWeight'])))/np.sum(np.array(fkp_gal_i['data/FKPWeight']))\n",
    "\n",
    "        sigma_p_g[i]=0.88*(sigma_v/100)*((1+zeff_g_i)/4)**(-0.4)\n",
    "        f_g[i]=cosmo.Om(zeff_g_i)**0.55\n",
    "        # Compute nonlinear matter power spectrum using Halofit\n",
    "        pnl_g_i=cosmology.HalofitPower(c, redshift=zeff_g_i)\n",
    "        matter_g_pk_pypower[i,:]=pnl_g_i(0.5 * (kedges[1:] + kedges[:-1]))\n",
    "        # Compute survey volume for normalization\n",
    "        chi_max_k=cosmolo.comoving_distance(np.max(np.array(gal_cat_arr_cat1_k[i]['z'])))*cosmolo.H0/100\n",
    "        chi_min_k=cosmolo.comoving_distance(np.min(np.array(gal_cat_arr_cat1_k[i]['z'])))*cosmolo.H0/100\n",
    "        V_w_k= (4 * np.pi/3) * ((6813)/41253) * (chi_max_k**3-chi_min_k**3)\n",
    "        # Compute covariance for the galaxy power spectrum\n",
    "        C_g_k[i,:]=(1/(V_w_k))*(((2*np.pi)**3)/(4*np.pi*((0.5 * (kedges[1:] + kedges[:-1]))**2)*dk))*(2*(gal_pk_pypower[i,:]+gal_pk_pypower_sn[i])**2)\n",
    "\n",
    "\n",
    "    gal_cat_pk_k_flat = gal_pk_pypower.flatten()\n",
    "    gal_cat_pk_k_flat.tofile('/gpfs/dsadathosseini/sdss_MZR_s50_dl4_dh05_newomcorr_file_final/gal_pk_pypower_M_c'+str(M_c))     \n",
    "    \n",
    "    C_g_k_flat = C_g_k.flatten()\n",
    "    C_g_k_flat.tofile('/gpfs/dsadathosseini/sdss_MZR_s50_dl4_dh05_newomcorr_file_final/C_g_k_M_c'+str(M_c))     \n",
    "    \n",
    "\n",
    "\n",
    "    #power of sirens\n",
    "\n",
    "\n",
    "\n",
    "    siren_pk_pypower=np.zeros((N_bins,m_bins, kbin))\n",
    "    siren_pk_pypower_sn = np.zeros((N_bins,m_bins))\n",
    "    FKP_siren_pypower=np.zeros((N_bins,m_bins))\n",
    "    power_gs=np.zeros((N_bins,m_bins,kbin))\n",
    "    bias_error=np.zeros((N_bins,m_bins,kbin))\n",
    "    C_s_k=np.zeros((N_bins,m_bins,kbin))\n",
    "    C_gs_k=np.zeros((N_bins,m_bins,kbin))\n",
    "    f_s=np.zeros((N_bins,m_bins))\n",
    "    data=siren_cat_arr_cat1_k\n",
    "    randoms=siren_cat_ran_arr_cat1_k\n",
    "    p_gg_numeric=np.zeros((N_bins, kbin))\n",
    "    p_ss_numeric=np.zeros((N_bins,m_bins,kbin))\n",
    "    b_gg_numeric=np.zeros((N_bins, kbin))\n",
    "    b_gm_rsd=np.zeros((N_bins, kbin,2))\n",
    "    b_ss_numeric=np.zeros((N_bins,m_bins,kbin))\n",
    "    bias_fit_g=np.zeros((N_bins,kbin))\n",
    "    bias_fit_s=np.zeros((N_bins,m_bins,kbin))\n",
    "    Z_eff = np.zeros((N_bins,m_bins))\n",
    "    for i in range(N_bins):\n",
    "        for j in range(m_bins):\n",
    "\n",
    "\n",
    "            ells = (0, 2, 4)\n",
    "            data=siren_cat_arr_cat1_k\n",
    "            randoms=siren_cat_ran_arr_cat1_k\n",
    "            if i==0 :\n",
    "                interval=np.linspace(0.07, 0.135, 100)\n",
    "                BoxSize = [272., 535., 300.]\n",
    "                Boxcenter = [-159.74, -11.47, 127.62]\n",
    "            elif i==1: \n",
    "                interval=np.linspace(0.135, 0.2, 100)\n",
    "                BoxSize = [544., 1040., 585.]\n",
    "                Boxcenter = [-304.22, -21.37, 247.3] \n",
    "\n",
    "            FSKY = 6813./41253. # a made-up value\n",
    "            # Compute redshift histogram for randoms to estimate selection function\n",
    "            zhist_g_i_k = RedshiftHistogram(gal_cat_ran_arr_cat1_k[i], FSKY, cosmolo, redshift='z',bins = interval)\n",
    "            zhist_s_i_j_k = RedshiftHistogram(siren_cat_ran_arr_cat1_k[i][j], FSKY, cosmolo, redshift='z',bins = interval)\n",
    "            alpha_s_i_j_k = 1.0 * siren_cat_arr_cat1_k[i][j].csize / siren_cat_ran_arr_cat1_k[i][j].csize\n",
    "            # Interpolate n(z) for siren data and randoms\n",
    "            nz_s_i_j = np.interp(np.array( siren_cat_arr_cat1_k[i][j]['z']),zhist_g_i_k.bin_centers,alpha_s_i_j_k*zhist_g_i_k.nbar)\n",
    "            nz_sr_i_j = np.interp(np.array(siren_cat_ran_arr_cat1_k[i][j]['z']),zhist_g_i_k.bin_centers,alpha_s_i_j_k*zhist_g_i_k.nbar)\n",
    "            # Build FKP catalog with weights for optimal power spectrum estimation\n",
    "            fkp_s_i_j = FKPCatalog(siren_cat_arr_cat1_k[i][j], siren_cat_ran_arr_cat1_k[i][j])\n",
    "            fkp_s_i_j['randoms/NZ'] = nz_sr_i_j\n",
    "            fkp_s_i_j['data/NZ'] = nz_s_i_j\n",
    "            fkp_s_i_j['data/FKPWeight'] = 1.0 / (1 + fkp_s_i_j['data/NZ'] * 1e4)\n",
    "            fkp_s_i_j['randoms/FKPWeight'] = 1.0 / (1 + fkp_s_i_j['randoms/NZ'] * 1e4)\n",
    "            # Compute weighted mean redshift for the siren bin\n",
    "            FKP_siren_pypower[i,j]=np.sum(np.array(fkp_s_i_j['data/FKPWeight']) * np.array(siren_cat_arr_cat1_k[i][j]['z']))/np.sum(np.array(fkp_s_i_j['data/FKPWeight']))\n",
    "\n",
    "            # Compute the power spectrum using FFT-based estimator for sirens\n",
    "            result_s_i_j = CatalogFFTPower(data_positions1=np.array(siren_cat_arr_cat1_k[i][j]['Position']), data_weights1=np.array(fkp_s_i_j['data/FKPWeight']),\n",
    "                                     randoms_positions1=np.array(siren_cat_ran_arr_cat1_k[i][j]['Position']), randoms_weights1=np.array(fkp_s_i_j['randoms/FKPWeight']),\n",
    "                                     edges=kedges, ells=ells, interlacing=2, boxsize=None,boxcenter=None, nmesh=256, resampler='tsc',\n",
    "                                     los='firstpoint', position_type='pos', mpiroot=0)\n",
    "            # Save the result for later analysis\n",
    "            result_s_i_j.save('/gpfs/dsadathosseini/sdss_MZR_s50_dl4_dh05_newomcorr_file_final/result_S_Mc'+str(M_c)+'_i'+str(i)+'_j'+str(j)) \n",
    "\n",
    "            Nmu = 1\n",
    "            mu_edges = numpy.linspace(0, 1, Nmu+1)\n",
    "            poles_s_i_j = result_s_i_j.poles\n",
    "            wedges_s_i_j= poles_s_i_j.to_wedges(muedges=mu_edges, ells=None)\n",
    "            siren_pk_pypower[i,j]=wedges_s_i_j.power[:,0]\n",
    "\n",
    "\n",
    "\n",
    "            siren_pk_pypower_sn[i,j]=shotnoise(wedges_s_i_j)\n",
    "            sigma_v=400\n",
    "            lower_limit_M = 0\n",
    "            upper_limit_M = 1\n",
    "            c = cosmology.Planck15\n",
    "            \n",
    "            zeff_s_ij=np.sum(np.array(siren_cat_arr_cat1_k[i][j]['z'] * np.array(fkp_s_i_j['data/FKPWeight'])))/np.sum(np.array(fkp_s_i_j['data/FKPWeight']))\n",
    "            Z_eff[i][j]=zeff_s_ij\n",
    "            Z_eff_flat = Z_eff.flatten()\n",
    "            Z_eff_flat.tofile('/gpfs/dsadathosseini/sdss_MZR_s50_dl4_dh05_newomcorr_file_final/Z_eff_Mc'+str(M_c))\n",
    " \n",
    "            \n",
    "            f_s[i,j]=cosmo.Om(zeff_s_ij)**0.55\n",
    "\n",
    "            sigma_p_s_ij=0.88*(sigma_v/100)*((1+zeff_s_ij)/4)**(-0.4)\n",
    "            b_lin_g_i=np.sqrt(gal_pk_pypower[i]/matter_g_pk_pypower[i])\n",
    "            b_lin_s_ij=np.sqrt(siren_pk_pypower[i,j]/matter_g_pk_pypower[i])\n",
    "\n",
    "            for k in range(kbin):\n",
    "                b_g = np.linspace(0,5,120)\n",
    "                b_s = np.linspace(0,5,120)\n",
    "                M = np.linspace(0, 1, 100)\n",
    "                dM = M[1]-M[0]\n",
    "\n",
    "\n",
    "                ratio_g=gal_pk_pypower[i][k]/matter_g_pk_pypower[i][k]\n",
    "                ratio_s=siren_pk_pypower[i][j][k]/matter_g_pk_pypower[i][k]\n",
    "\n",
    "                integrand_g = (b_g**2+2*b_g*f_g[i]*M[:,np.newaxis]**2+f_g[i]**2*M[:,np.newaxis]**4)*(1/(1+((0.5 * (kedges[1:] + kedges[:-1])[k])**2*M[:,np.newaxis]**2*sigma_p_g[i]**2/2)))\n",
    "                integrand_s = (b_s**2+2*b_s*f_s[i,j]*M[:,np.newaxis]**2+f_s[i,j]**2*M[:,np.newaxis]**4)*(1/(1+((0.5 * (kedges[1:] + kedges[:-1])[k])**2*M[:,np.newaxis]**2*sigma_p_s_ij**2/2)))\n",
    "\n",
    "                integral_g = np.sum(integrand_g * dM, axis=0)\n",
    "                integral_s = np.sum(integrand_s * dM, axis=0)\n",
    "\n",
    "                bias_fit_g[i][k]=b_g[np.argmin((ratio_g-integral_g)**2)]\n",
    "                bias_fit_s[i][j][k]=b_s[np.argmin((ratio_s-integral_s)**2)]\n",
    "\n",
    "                power_gs[i][j][k]=integrate.quad(rsd_p_gs,0,1,args=(bias_fit_g[i][k],bias_fit_s[i][j][k],matter_g_pk_pypower[i][k],f_s[i,j],0.5 * (kedges[1:] + kedges[:-1])[k],sigma_p_s_ij))[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            p_gg_numeric[i]=bias_fit_g[i]*matter_g_pk_pypower[i]\n",
    "            p_ss_numeric[i][j]=bias_fit_s[i][j]*matter_g_pk_pypower[i]\n",
    "\n",
    "\n",
    "            chi_max_g=cosmolo.comoving_distance(np.max(np.array(gal_cat_arr_cat1_k[i]['z'])))*cosmolo.H0/100\n",
    "            chi_min_g=cosmolo.comoving_distance(np.min(np.array(gal_cat_arr_cat1_k[i]['z'])))*cosmolo.H0/100\n",
    "            V_w_k_g= (4 * np.pi/3) * ((6813)/41253) * (chi_max_g**3-chi_min_g**3)\n",
    "\n",
    "            C_g_k[i,:]=(1/(V_w_k_g))*(((2*np.pi)**3)/(4*np.pi*((0.5 * (kedges[1:] + kedges[:-1]))**2)*dk))*(2*(gal_pk_pypower[i,:]+gal_pk_pypower_sn[i])**2)\n",
    "\n",
    "\n",
    "            chi_maxs_s=cosmolo.comoving_distance(np.max(np.array(siren_cat_arr_cat1_k[i][j]['z'])))*cosmolo.H0/100\n",
    "            chi_mins_s=cosmolo.comoving_distance(np.min(np.array(siren_cat_arr_cat1_k[i][j]['z'])))*cosmolo.H0/100\n",
    "            V_w_k_s= (4 * np.pi/3) * ((6813)/41253) * (chi_maxs_s**3-chi_mins_s**3)\n",
    "            C_s_k[i,j,:]=(1/(V_w_k_s))*(((2*np.pi)**3)/(4*np.pi*((0.5 * (kedges[1:] + kedges[:-1]))**2)*dk))*(2*(siren_pk_pypower[i][j]+siren_pk_pypower_sn[i,j])**2)\n",
    "\n",
    "\n",
    "\n",
    "            C_gs_k[i,j,:] = (1/(V_w_k))*(((2*np.pi)**3)/(4*np.pi*((0.5 * (kedges[1:] + kedges[:-1]))**2)*dk))* (2 * (power_gs[i,j] + gal_pk_pypower_sn[i])**2)\n",
    "\n",
    "\n",
    "            bias_error[i,j,:]=np.sqrt((1/(gal_pk_pypower[i])**2)*(C_s_k[i,j])+((siren_pk_pypower[i][j]/(gal_pk_pypower[i])**2)**2)*(C_g_k[i,:])-2*((siren_pk_pypower[i][j])/((gal_pk_pypower[i])**3))*C_gs_k[i,j,:])\n",
    "\n",
    "\n",
    "    siren_cat_pk_k_flat = siren_pk_pypower.flatten()\n",
    "    siren_cat_pk_k_flat.tofile('/gpfs/dsadathosseini/sdss_MZR_s50_dl4_dh05_newomcorr_file_final/siren_pk_pypower_M_c'+str(M_c))\n",
    "    bias_error_flat = bias_error.flatten()\n",
    "    bias_error_flat.tofile('/gpfs/dsadathosseini/sdss_MZR_s50_dl4_dh05_newomcorr_file_final/bias_error_M_c'+str(M_c))\n",
    "    C_s_k_flat = C_s_k.flatten()\n",
    "    C_s_k_flat.tofile('/gpfs/dsadathosseini/sdss_MZR_s50_dl4_dh05_newomcorr_file_final/C_s_k_M_c'+str(M_c))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gw_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
